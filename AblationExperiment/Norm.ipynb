{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-03T11:00:22.939165700Z",
     "start_time": "2024-01-03T11:00:20.657065100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227406, 100, 3)\n",
      "(227406,)\n"
     ]
    }
   ],
   "source": [
    "class my_Dataset(Data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = features\n",
    "        self.y = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "class WheelRailTrajectoryDataset:\n",
    "    def __init__(self,label=3):\n",
    "        standard_scaler = StandardScaler()\n",
    "        self.raw_data1 = pd.read_csv(\"../dataset/curve_R400.csv\")\n",
    "        self.raw_data2 = pd.read_csv(\"../dataset/curve_R1000.csv\")\n",
    "        self.raw_data3 = pd.read_csv(\"../dataset/curve_R2000.csv\")\n",
    "        self.raw_data4 = pd.read_csv(\"../dataset/curve_R3000.csv\")\n",
    "        self.raw_data5 = pd.read_csv(\"../dataset/curve_R4000.csv\")\n",
    "        self.raw_data6 = pd.read_csv(\"../dataset/curve_R5000.csv\")\n",
    "        self.raw_data = pd.concat([self.raw_data1,self.raw_data2,self.raw_data3,self.raw_data4,self.raw_data5,self.raw_data6])\n",
    "        self.label = label\n",
    "        #标准化\n",
    "        self.data = standard_scaler.fit_transform(self.raw_data)\n",
    "        self.mean = standard_scaler.mean_[label]\n",
    "        self.sd = math.sqrt(standard_scaler.var_[label])\n",
    "\n",
    "\n",
    "    def construct_set(self, train_por=0.6,val_por=0.2,test_por=0.2, window_size=100):\n",
    "        X = []\n",
    "        Y = []\n",
    "        list = [self.raw_data1.shape[0],self.raw_data2.shape[0],self.raw_data3.shape[0],self.raw_data4.shape[0],self.raw_data5.shape[0],self.raw_data6.shape[0]]\n",
    "        num = 0\n",
    "        for j in range(len(list)):\n",
    "            if j!=0:\n",
    "                num = num+list[j-1]\n",
    "            for i in range(list[j]-window_size):\n",
    "                seq = self.data[num+i:num+i+window_size+1]\n",
    "                X.append(seq[0:window_size,:self.label])\n",
    "                y = seq[window_size:window_size+1,self.label]\n",
    "                if(y>0.6 or y<-0.6):\n",
    "                    Y.append(1)\n",
    "                else:\n",
    "                    Y.append(0)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        print(X.shape)\n",
    "        print(Y.shape)\n",
    "        train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=test_por,train_size=train_por+val_por,shuffle=True)\n",
    "        train_x,val_x,train_y,val_y =  train_test_split(train_x,train_y,test_size=val_por/(val_por+train_por),train_size=train_por/(val_por+train_por),shuffle=True)\n",
    "\n",
    "        train_set = my_Dataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "        val_set = my_Dataset(torch.Tensor(val_x), torch.Tensor(val_y))\n",
    "        test_set = my_Dataset(torch.Tensor(test_x), torch.Tensor(test_y))\n",
    "        return train_set, val_set, test_set\n",
    "\n",
    "TrajectoryData = WheelRailTrajectoryDataset()\n",
    "train_set, val_set, test_set = TrajectoryData.construct_set()\n",
    "batch_size = 256\n",
    "train_loader = Data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = Data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "test_loader = Data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T11:00:31.898626800Z",
     "start_time": "2024-01-03T11:00:30.922879200Z"
    }
   },
   "id": "eb3d4dce34f4bcf3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, loss,device=\"cuda\"):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        l = loss(y_hat, y.long()).sum()\n",
    "        test_l_sum += l.item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum/n, test_l_sum/n\n",
    "\n",
    "def visualization(train_loss,val_loss,train_accuracy,val_accuracy):\n",
    "    plt.figure(1)\n",
    "    x1=np.linspace(0,len(train_loss),len(train_loss))\n",
    "    plt.plot(x1,train_loss,label='train_loss',linewidth=1.5)\n",
    "    plt.plot(x1,val_loss,label='test_loss',linewidth=1.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(2)\n",
    "    x2 = np.linspace(0,len(train_accuracy),len(train_accuracy))\n",
    "    plt.plot(x2,train_accuracy,label='train_accuracy',linewidth=1.5)\n",
    "    plt.plot(x2,val_accuracy,label='val_accuracy',linewidth=1.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_PR(data_iter, net, loss,device=\"cuda\"):\n",
    "    n = 0\n",
    "    precision_sum,recall_sum,f1_score_sum = 0.0,0.0,0.0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y.long()).sum()\n",
    "        y_hat = y_hat.argmax(dim=1)\n",
    "        y_hat = y_hat.squeeze().cpu().detach().numpy()\n",
    "        y = y.cpu().detach().numpy()\n",
    "        precision_sum += metrics.precision_score(y,y_hat)\n",
    "        recall_sum += metrics.recall_score(y,y_hat)\n",
    "        f1_score_sum += metrics.f1_score(y,y_hat)\n",
    "        test_l_sum += l.item()\n",
    "        #n += y.shape[0]\n",
    "        n+=1\n",
    "    return precision_sum/n,recall_sum/n,f1_score_sum/n\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self,seq_len,channel_dim,P=1,S=1):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.emb = nn.Conv1d(in_channels=1, out_channels=channel_dim,\n",
    "                             kernel_size=P,padding=0,stride=S,groups=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #input b s v\n",
    "        b = x.shape[0]\n",
    "        x = x.permute(0,2,1) # b v s\n",
    "        x = x.unsqueeze(2)#B V 1 S\n",
    "        x = rearrange(x, 'b v c s -> (b v) c s')# B*V 1 S\n",
    "        out = self.emb(x) # b v c s\n",
    "        out = rearrange(out, '(b v) c s -> b v c s', b=b)\n",
    "        return out # b v c s\n",
    "device = torch.device(\"cuda\")\n",
    "lr = 0.0001\n",
    "epochs=300\n",
    "loss=nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T11:00:38.796693300Z",
     "start_time": "2024-01-03T11:00:38.779497200Z"
    }
   },
   "id": "9d91a8e39bccf76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.LayerNorm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a69cc79d26c87cd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ..\\aten\\src\\ATen\\native\\Convolution.cpp:1009.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.0020,train acc 0.757100,val acc 0.762\n",
      "epoch2,loss0.0019,train acc 0.760404,val acc 0.762\n",
      "epoch3,loss0.0018,train acc 0.762291,val acc 0.777\n",
      "epoch4,loss0.0017,train acc 0.867467,val acc 0.892\n",
      "epoch5,loss0.0016,train acc 0.887967,val acc 0.896\n",
      "epoch6,loss0.0016,train acc 0.894722,val acc 0.901\n",
      "epoch7,loss0.0016,train acc 0.900780,val acc 0.898\n",
      "epoch8,loss0.0016,train acc 0.904686,val acc 0.910\n",
      "epoch9,loss0.0016,train acc 0.911221,val acc 0.896\n",
      "epoch10,loss0.0016,train acc 0.914165,val acc 0.918\n",
      "epoch11,loss0.0015,train acc 0.915259,val acc 0.908\n",
      "epoch12,loss0.0015,train acc 0.920355,val acc 0.921\n",
      "epoch13,loss0.0015,train acc 0.923997,val acc 0.925\n",
      "epoch14,loss0.0015,train acc 0.925730,val acc 0.931\n",
      "epoch15,loss0.0015,train acc 0.927962,val acc 0.934\n",
      "epoch16,loss0.0015,train acc 0.930282,val acc 0.932\n",
      "epoch17,loss0.0015,train acc 0.932948,val acc 0.927\n",
      "epoch18,loss0.0015,train acc 0.934783,val acc 0.935\n",
      "epoch19,loss0.0015,train acc 0.937757,val acc 0.930\n",
      "epoch20,loss0.0015,train acc 0.938176,val acc 0.944\n",
      "epoch21,loss0.0015,train acc 0.941193,val acc 0.934\n",
      "epoch22,loss0.0015,train acc 0.939842,val acc 0.939\n",
      "epoch23,loss0.0014,train acc 0.941568,val acc 0.934\n",
      "epoch24,loss0.0014,train acc 0.942427,val acc 0.948\n",
      "epoch25,loss0.0014,train acc 0.945804,val acc 0.948\n",
      "epoch26,loss0.0014,train acc 0.944960,val acc 0.941\n",
      "epoch27,loss0.0014,train acc 0.946267,val acc 0.950\n",
      "epoch28,loss0.0014,train acc 0.946487,val acc 0.950\n",
      "epoch29,loss0.0014,train acc 0.949292,val acc 0.937\n",
      "epoch30,loss0.0014,train acc 0.952281,val acc 0.954\n",
      "epoch31,loss0.0014,train acc 0.951451,val acc 0.951\n",
      "epoch32,loss0.0014,train acc 0.951840,val acc 0.952\n",
      "epoch33,loss0.0014,train acc 0.951627,val acc 0.955\n",
      "epoch34,loss0.0014,train acc 0.953639,val acc 0.947\n",
      "epoch35,loss0.0014,train acc 0.954696,val acc 0.956\n",
      "epoch36,loss0.0014,train acc 0.955746,val acc 0.956\n",
      "epoch37,loss0.0014,train acc 0.957406,val acc 0.956\n",
      "epoch38,loss0.0014,train acc 0.956539,val acc 0.953\n",
      "epoch39,loss0.0014,train acc 0.957325,val acc 0.928\n",
      "epoch40,loss0.0014,train acc 0.956782,val acc 0.956\n",
      "epoch41,loss0.0014,train acc 0.958625,val acc 0.959\n",
      "epoch42,loss0.0014,train acc 0.959278,val acc 0.958\n",
      "epoch43,loss0.0014,train acc 0.960747,val acc 0.964\n",
      "epoch44,loss0.0014,train acc 0.960255,val acc 0.955\n",
      "epoch45,loss0.0014,train acc 0.960071,val acc 0.956\n",
      "epoch46,loss0.0014,train acc 0.960578,val acc 0.933\n",
      "epoch47,loss0.0014,train acc 0.962766,val acc 0.959\n",
      "epoch48,loss0.0014,train acc 0.961407,val acc 0.960\n",
      "epoch49,loss0.0014,train acc 0.963353,val acc 0.960\n",
      "epoch50,loss0.0014,train acc 0.963764,val acc 0.949\n",
      "epoch51,loss0.0014,train acc 0.964947,val acc 0.964\n",
      "epoch52,loss0.0014,train acc 0.963963,val acc 0.957\n",
      "epoch53,loss0.0014,train acc 0.964506,val acc 0.966\n",
      "epoch54,loss0.0014,train acc 0.965637,val acc 0.965\n",
      "epoch55,loss0.0014,train acc 0.966114,val acc 0.966\n",
      "epoch56,loss0.0014,train acc 0.965651,val acc 0.958\n",
      "epoch57,loss0.0014,train acc 0.966738,val acc 0.965\n",
      "epoch58,loss0.0014,train acc 0.966628,val acc 0.968\n",
      "epoch59,loss0.0014,train acc 0.966760,val acc 0.968\n",
      "epoch60,loss0.0013,train acc 0.969616,val acc 0.969\n",
      "epoch61,loss0.0014,train acc 0.968148,val acc 0.970\n",
      "epoch62,loss0.0013,train acc 0.968948,val acc 0.971\n",
      "epoch63,loss0.0013,train acc 0.968544,val acc 0.965\n",
      "epoch64,loss0.0013,train acc 0.967964,val acc 0.945\n",
      "epoch65,loss0.0013,train acc 0.969925,val acc 0.969\n",
      "epoch66,loss0.0013,train acc 0.970211,val acc 0.961\n",
      "epoch67,loss0.0013,train acc 0.968970,val acc 0.965\n",
      "epoch68,loss0.0013,train acc 0.969616,val acc 0.968\n",
      "epoch69,loss0.0013,train acc 0.972120,val acc 0.969\n",
      "epoch70,loss0.0013,train acc 0.971335,val acc 0.965\n",
      "epoch71,loss0.0013,train acc 0.971687,val acc 0.972\n",
      "epoch72,loss0.0013,train acc 0.970431,val acc 0.969\n",
      "epoch73,loss0.0013,train acc 0.971437,val acc 0.974\n",
      "epoch74,loss0.0013,train acc 0.971239,val acc 0.970\n",
      "epoch75,loss0.0013,train acc 0.972370,val acc 0.965\n",
      "epoch76,loss0.0013,train acc 0.973493,val acc 0.970\n",
      "epoch77,loss0.0013,train acc 0.972634,val acc 0.966\n",
      "epoch78,loss0.0013,train acc 0.972649,val acc 0.973\n",
      "epoch79,loss0.0013,train acc 0.973949,val acc 0.972\n",
      "epoch80,loss0.0013,train acc 0.973442,val acc 0.972\n",
      "epoch81,loss0.0013,train acc 0.974426,val acc 0.971\n",
      "epoch82,loss0.0013,train acc 0.975175,val acc 0.968\n",
      "epoch83,loss0.0013,train acc 0.974147,val acc 0.974\n",
      "epoch84,loss0.0013,train acc 0.973971,val acc 0.965\n",
      "epoch85,loss0.0013,train acc 0.975138,val acc 0.975\n",
      "epoch86,loss0.0013,train acc 0.975417,val acc 0.968\n",
      "epoch87,loss0.0013,train acc 0.976041,val acc 0.964\n",
      "epoch88,loss0.0013,train acc 0.975402,val acc 0.976\n",
      "epoch89,loss0.0013,train acc 0.975843,val acc 0.974\n",
      "epoch90,loss0.0013,train acc 0.976298,val acc 0.976\n",
      "epoch91,loss0.0013,train acc 0.976188,val acc 0.973\n",
      "epoch92,loss0.0013,train acc 0.977833,val acc 0.977\n",
      "epoch93,loss0.0013,train acc 0.976673,val acc 0.975\n",
      "epoch94,loss0.0013,train acc 0.976886,val acc 0.976\n",
      "epoch95,loss0.0013,train acc 0.977965,val acc 0.966\n",
      "epoch96,loss0.0013,train acc 0.977473,val acc 0.974\n",
      "epoch97,loss0.0013,train acc 0.977319,val acc 0.970\n",
      "epoch98,loss0.0013,train acc 0.978244,val acc 0.972\n",
      "epoch99,loss0.0013,train acc 0.978097,val acc 0.977\n",
      "epoch100,loss0.0013,train acc 0.976761,val acc 0.977\n",
      "epoch101,loss0.0013,train acc 0.978273,val acc 0.976\n",
      "epoch102,loss0.0013,train acc 0.978596,val acc 0.977\n",
      "epoch103,loss0.0013,train acc 0.977054,val acc 0.965\n",
      "epoch104,loss0.0013,train acc 0.977803,val acc 0.977\n",
      "epoch105,loss0.0013,train acc 0.979779,val acc 0.973\n",
      "epoch106,loss0.0013,train acc 0.978765,val acc 0.974\n",
      "epoch107,loss0.0013,train acc 0.978662,val acc 0.972\n",
      "epoch108,loss0.0013,train acc 0.979022,val acc 0.979\n",
      "epoch109,loss0.0013,train acc 0.979382,val acc 0.962\n",
      "epoch110,loss0.0013,train acc 0.979433,val acc 0.974\n",
      "epoch111,loss0.0013,train acc 0.978604,val acc 0.978\n",
      "epoch112,loss0.0013,train acc 0.979235,val acc 0.970\n",
      "epoch113,loss0.0013,train acc 0.979727,val acc 0.975\n",
      "epoch114,loss0.0013,train acc 0.980234,val acc 0.976\n",
      "epoch115,loss0.0013,train acc 0.978244,val acc 0.978\n",
      "epoch116,loss0.0013,train acc 0.979147,val acc 0.977\n",
      "epoch117,loss0.0013,train acc 0.980058,val acc 0.973\n",
      "epoch118,loss0.0013,train acc 0.979801,val acc 0.973\n",
      "epoch119,loss0.0013,train acc 0.980102,val acc 0.977\n",
      "epoch120,loss0.0013,train acc 0.979661,val acc 0.977\n",
      "epoch121,loss0.0013,train acc 0.980527,val acc 0.971\n",
      "epoch122,loss0.0013,train acc 0.980909,val acc 0.980\n",
      "epoch123,loss0.0013,train acc 0.981049,val acc 0.979\n",
      "epoch124,loss0.0013,train acc 0.980858,val acc 0.976\n",
      "epoch125,loss0.0013,train acc 0.982136,val acc 0.977\n",
      "epoch126,loss0.0013,train acc 0.980902,val acc 0.979\n",
      "epoch127,loss0.0013,train acc 0.981776,val acc 0.980\n",
      "epoch128,loss0.0013,train acc 0.981078,val acc 0.976\n",
      "epoch129,loss0.0013,train acc 0.981306,val acc 0.980\n",
      "epoch130,loss0.0013,train acc 0.980997,val acc 0.980\n",
      "epoch131,loss0.0013,train acc 0.982150,val acc 0.980\n",
      "epoch132,loss0.0013,train acc 0.982723,val acc 0.977\n",
      "epoch133,loss0.0013,train acc 0.982069,val acc 0.980\n",
      "epoch134,loss0.0013,train acc 0.979500,val acc 0.976\n",
      "epoch135,loss0.0013,train acc 0.981746,val acc 0.977\n",
      "epoch136,loss0.0013,train acc 0.981834,val acc 0.973\n",
      "epoch137,loss0.0013,train acc 0.982840,val acc 0.978\n",
      "epoch138,loss0.0013,train acc 0.982370,val acc 0.982\n",
      "epoch139,loss0.0013,train acc 0.982018,val acc 0.978\n",
      "epoch140,loss0.0013,train acc 0.982657,val acc 0.979\n",
      "epoch141,loss0.0013,train acc 0.982833,val acc 0.974\n",
      "epoch142,loss0.0013,train acc 0.981577,val acc 0.973\n",
      "epoch143,loss0.0013,train acc 0.982635,val acc 0.979\n",
      "epoch144,loss0.0013,train acc 0.982789,val acc 0.982\n",
      "epoch145,loss0.0013,train acc 0.981357,val acc 0.974\n",
      "epoch146,loss0.0013,train acc 0.983149,val acc 0.975\n",
      "epoch147,loss0.0013,train acc 0.983259,val acc 0.975\n",
      "epoch148,loss0.0013,train acc 0.983391,val acc 0.981\n",
      "epoch149,loss0.0013,train acc 0.982459,val acc 0.979\n",
      "epoch150,loss0.0013,train acc 0.982958,val acc 0.982\n",
      "epoch151,loss0.0013,train acc 0.982973,val acc 0.981\n",
      "epoch152,loss0.0013,train acc 0.983442,val acc 0.979\n",
      "epoch153,loss0.0013,train acc 0.984125,val acc 0.975\n",
      "epoch154,loss0.0013,train acc 0.983281,val acc 0.977\n",
      "epoch155,loss0.0013,train acc 0.982503,val acc 0.979\n",
      "epoch156,loss0.0013,train acc 0.983369,val acc 0.980\n",
      "epoch157,loss0.0013,train acc 0.984096,val acc 0.981\n",
      "epoch158,loss0.0013,train acc 0.983758,val acc 0.978\n",
      "epoch159,loss0.0013,train acc 0.984676,val acc 0.979\n",
      "epoch160,loss0.0013,train acc 0.983633,val acc 0.978\n",
      "epoch161,loss0.0013,train acc 0.983281,val acc 0.980\n",
      "epoch162,loss0.0013,train acc 0.984889,val acc 0.979\n",
      "epoch163,loss0.0013,train acc 0.983362,val acc 0.966\n",
      "epoch164,loss0.0013,train acc 0.984581,val acc 0.981\n",
      "epoch165,loss0.0013,train acc 0.984874,val acc 0.976\n",
      "epoch166,loss0.0013,train acc 0.984338,val acc 0.982\n",
      "epoch167,loss0.0013,train acc 0.984727,val acc 0.983\n",
      "epoch168,loss0.0013,train acc 0.984823,val acc 0.981\n",
      "epoch169,loss0.0013,train acc 0.984500,val acc 0.979\n",
      "epoch170,loss0.0013,train acc 0.984852,val acc 0.978\n",
      "epoch171,loss0.0013,train acc 0.985065,val acc 0.978\n",
      "epoch172,loss0.0013,train acc 0.985352,val acc 0.981\n",
      "epoch173,loss0.0013,train acc 0.984478,val acc 0.981\n",
      "epoch174,loss0.0013,train acc 0.985689,val acc 0.981\n",
      "epoch175,loss0.0013,train acc 0.984404,val acc 0.982\n",
      "epoch176,loss0.0013,train acc 0.984683,val acc 0.982\n",
      "epoch177,loss0.0013,train acc 0.985330,val acc 0.974\n",
      "epoch178,loss0.0013,train acc 0.984456,val acc 0.980\n",
      "epoch179,loss0.0013,train acc 0.985866,val acc 0.980\n",
      "epoch180,loss0.0013,train acc 0.984507,val acc 0.975\n",
      "epoch181,loss0.0013,train acc 0.985234,val acc 0.983\n",
      "epoch182,loss0.0013,train acc 0.984882,val acc 0.980\n",
      "epoch183,loss0.0013,train acc 0.985609,val acc 0.980\n",
      "epoch184,loss0.0013,train acc 0.985748,val acc 0.971\n",
      "epoch185,loss0.0013,train acc 0.986078,val acc 0.981\n",
      "epoch186,loss0.0013,train acc 0.985946,val acc 0.976\n",
      "epoch187,loss0.0013,train acc 0.985476,val acc 0.979\n",
      "epoch188,loss0.0013,train acc 0.985432,val acc 0.979\n",
      "epoch189,loss0.0013,train acc 0.985968,val acc 0.982\n",
      "epoch190,loss0.0013,train acc 0.986343,val acc 0.981\n",
      "epoch191,loss0.0013,train acc 0.985645,val acc 0.984\n",
      "epoch192,loss0.0013,train acc 0.985337,val acc 0.974\n",
      "epoch193,loss0.0013,train acc 0.986174,val acc 0.977\n",
      "epoch194,loss0.0013,train acc 0.986291,val acc 0.984\n",
      "epoch195,loss0.0013,train acc 0.986864,val acc 0.982\n",
      "epoch196,loss0.0013,train acc 0.986409,val acc 0.982\n",
      "epoch197,loss0.0013,train acc 0.985675,val acc 0.982\n",
      "epoch198,loss0.0013,train acc 0.986813,val acc 0.982\n",
      "epoch199,loss0.0013,train acc 0.985792,val acc 0.984\n",
      "epoch200,loss0.0013,train acc 0.987554,val acc 0.980\n",
      "epoch201,loss0.0013,train acc 0.986798,val acc 0.978\n",
      "epoch202,loss0.0013,train acc 0.986798,val acc 0.983\n",
      "epoch203,loss0.0013,train acc 0.986952,val acc 0.981\n",
      "epoch204,loss0.0013,train acc 0.986034,val acc 0.983\n",
      "epoch205,loss0.0013,train acc 0.986291,val acc 0.982\n",
      "epoch206,loss0.0013,train acc 0.986563,val acc 0.981\n",
      "epoch207,loss0.0013,train acc 0.986695,val acc 0.984\n",
      "epoch208,loss0.0013,train acc 0.986835,val acc 0.984\n",
      "epoch209,loss0.0013,train acc 0.986871,val acc 0.981\n",
      "epoch210,loss0.0013,train acc 0.987297,val acc 0.982\n",
      "epoch211,loss0.0013,train acc 0.986269,val acc 0.983\n",
      "epoch212,loss0.0013,train acc 0.987562,val acc 0.983\n",
      "epoch213,loss0.0013,train acc 0.987077,val acc 0.979\n"
     ]
    }
   ],
   "source": [
    "class ModernCBlock(nn.Module):\n",
    "    def __init__(self,variable_dim,r,seq_len,channel_dim):\n",
    "        super(ModernCBlock,self).__init__()\n",
    "        #conv_s model seq\n",
    "        self.conv_b1 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//2,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b2 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//4,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b3 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//5,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b4 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//10,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "\n",
    "        #ffn_c model channel\n",
    "        self.ffn_c = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=variable_dim*channel_dim, out_channels=r*variable_dim*channel_dim,padding='same',kernel_size=1,groups=variable_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels=r*variable_dim*channel_dim, out_channels=variable_dim*channel_dim,padding='same',kernel_size=1,groups=variable_dim)\n",
    "        )\n",
    "\n",
    "        #ffn_v model variable\n",
    "        self.ffn_v = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=variable_dim*channel_dim, out_channels=r*variable_dim*channel_dim,padding='same',kernel_size=1,groups=channel_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels=r*variable_dim*channel_dim, out_channels=variable_dim*channel_dim,padding='same',kernel_size=1,groups=channel_dim)\n",
    "        )\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(variable_dim*channel_dim)\n",
    "        self.norm_2 = nn.LayerNorm(variable_dim*channel_dim)\n",
    "        self.norm_3 = nn.LayerNorm(channel_dim*variable_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #input : b v c s\n",
    "        v = x.shape[1]\n",
    "        x = rearrange(x, 'b v c s -> b (v c) s')\n",
    "        residual = x\n",
    "        y1 = self.conv_b1(x)\n",
    "        y2 = self.conv_b2(x)\n",
    "        y3 = self.conv_b3(x)\n",
    "        y4 = self.conv_b4(x)\n",
    "        y = y1 + y2 +y3 + y4 + residual\n",
    "        y = y.permute(0,2,1)\n",
    "        y = self.norm_1(y)\n",
    "        y = y.permute(0,2,1)\n",
    "        y = rearrange(y,'b (v c) s -> b v c s', v=v)\n",
    "\n",
    "        y = rearrange(y, 'b v c s -> b (v c) s')\n",
    "        residual = y\n",
    "        z = self.ffn_c(y) # \n",
    "        z = z + residual\n",
    "        z = z.permute(0,2,1)\n",
    "        z = self.norm_2(z)\n",
    "        z = z.permute(0,2,1)\n",
    "        z = rearrange(z,'b (v c) s -> b v c s', v=v)\n",
    "        z = z.permute(0,2,1,3)\n",
    "\n",
    "        z = rearrange(z, 'b c v s -> b (c v) s')\n",
    "        residual = z\n",
    "        out = self.ffn_v(z)\n",
    "        out = out + residual\n",
    "        out = out.permute(0,2,1)\n",
    "        out = self.norm_3(out)\n",
    "        out = out.permute(0,2,1)\n",
    "        out = rearrange(out,'b (c v) s -> b c v s',v=v)\n",
    "        out = out.permute(0,2,1,3)# b v c s\n",
    "        return out\n",
    "\n",
    "class ModernCNet(nn.Module):\n",
    "\n",
    "    def __init__(self,num_block,r,seq_len,channel_dim,variable_dim):\n",
    "        super(ModernCNet,self).__init__()\n",
    "        self.emb = Embedding(seq_len,channel_dim)\n",
    "        self.ModernCList = nn.ModuleList([ModernCBlock(variable_dim,r,seq_len,channel_dim) for _ in range(num_block)])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=variable_dim*seq_len*channel_dim,out_features=variable_dim*seq_len*channel_dim//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=variable_dim*seq_len*channel_dim//2,out_features=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.emb(x) # b v c s\n",
    "        #print(\"x:\",x.shape) #256 3 6 50\n",
    "        residual = x # b v c s\n",
    "        features = x\n",
    "        for block in self.ModernCList:\n",
    "            features = block(features)\n",
    "        features = features + residual\n",
    "        features = self.flatten(features) # b (v*c*s)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "\n",
    "def train(net,train_iter,val_iter,loss,num_epochs,optimizer,device=\"cuda\"):\n",
    "    net.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    val_acc_best = 1e-9\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            #print(\"X:\",X.shape)\n",
    "            y = y.to(device)\n",
    "            #print(\"y:\",y.shape) \n",
    "            y_hat = net(X)\n",
    "            #print(\"y_hat:\",y_hat.shape)\n",
    "            #y_hat = y_hat.squeeze()\n",
    "            #print(\"y_hat_squeeze:\",y_hat.shape)\n",
    "            l=loss(y_hat,y.long()).sum()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        net.eval()\n",
    "        val_acc,val_l = evaluate_accuracy(val_iter,net,loss)\n",
    "        net.train()\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        val_loss.append(val_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        val_accuracy.append(val_acc)\n",
    "        if(val_acc>val_acc_best):\n",
    "            val_acc_best = val_acc\n",
    "            train_loss_best = train_l_sum/n\n",
    "            train_acc_best = train_acc_sum/n\n",
    "            epoch_best = epoch\n",
    "            torch.save(net.state_dict(),\"weights/DCCN_LN_best.pt\")\n",
    "        print('epoch%d,loss%.4f,train acc %3f,val acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,val_acc))\n",
    "    print(\"best weight was found in epoch%d,where the train loss is %.4f,train acc is %3f,val acc is %.3f\"%(epoch_best,train_loss_best,train_acc_best,val_acc_best))\n",
    "    return train_loss,val_loss,train_accuracy,val_accuracy\n",
    "\n",
    "net = ModernCNet(3,2,100,6,3)\n",
    "net.to(device)\n",
    "optimizer=optim.Adam(net.parameters(),lr)\n",
    "\n",
    "train_loss,val_loss,train_accuracy,val_accuracy=train(net,train_loader,val_loader,loss,epochs,optimizer)\n",
    "visualization(train_loss,val_loss,train_accuracy,val_accuracy)\n",
    "net.load_state_dict(torch.load(\"weights/DCCN_LN_best.pt\"))\n",
    "test_acc,_ = evaluate_accuracy(test_loader,net,loss)\n",
    "print(\"Accuracy on test set : %.3f\"%(test_acc))\n",
    "test_precision,test_recall,test_f1_score = evaluate_PR(test_loader,net,loss)\n",
    "print(\"test precision:\",test_precision,\" test recall:\",test_recall,\" test f1_score:\",test_f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-03T11:00:58.876932100Z"
    }
   },
   "id": "a0a078a89a09aa26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.BatchNorm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bbc237db02162b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ModernCBlock(nn.Module):\n",
    "    def __init__(self,variable_dim,r,seq_len,channel_dim):\n",
    "        super(ModernCBlock,self).__init__()\n",
    "        #conv_s model seq\n",
    "        self.conv_b1 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//2,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b2 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//4,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b3 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//5,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b4 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//10,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "\n",
    "        #ffn_c model channel\n",
    "        self.ffn_c = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=variable_dim*channel_dim, out_channels=r*variable_dim*channel_dim,padding='same',kernel_size=1,groups=variable_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels=r*variable_dim*channel_dim, out_channels=variable_dim*channel_dim,padding='same',kernel_size=1,groups=variable_dim)\n",
    "        )\n",
    "\n",
    "        #ffn_v model variable\n",
    "        self.ffn_v = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=variable_dim*channel_dim, out_channels=r*variable_dim*channel_dim,padding='same',kernel_size=1,groups=channel_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels=r*variable_dim*channel_dim, out_channels=variable_dim*channel_dim,padding='same',kernel_size=1,groups=channel_dim)\n",
    "        )\n",
    "\n",
    "        self.norm_1 = nn.BatchNorm1d(variable_dim*channel_dim)\n",
    "        self.norm_2 = nn.BatchNorm1d(variable_dim*channel_dim)\n",
    "        self.norm_3 = nn.BatchNorm1d(channel_dim*variable_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #input : b v c s\n",
    "        v = x.shape[1]\n",
    "        x = rearrange(x, 'b v c s -> b (v c) s')\n",
    "        residual = x\n",
    "        y1 = self.conv_b1(x)\n",
    "        y2 = self.conv_b2(x)\n",
    "        y3 = self.conv_b3(x)\n",
    "        y4 = self.conv_b4(x)\n",
    "        y = y1 + y2 +y3 + y4 + residual\n",
    "        y = self.norm_1(y)\n",
    "        y = rearrange(y,'b (v c) s -> b v c s', v=v)\n",
    "        y = rearrange(y, 'b v c s -> b (v c) s')\n",
    "        residual = y\n",
    "        z = self.ffn_c(y) # \n",
    "        z = z + residual\n",
    "        z = self.norm_2(z)\n",
    "        z = rearrange(z,'b (v c) s -> b v c s', v=v)\n",
    "        z = z.permute(0,2,1,3)\n",
    "        z = rearrange(z, 'b c v s -> b (c v) s')\n",
    "        residual = z\n",
    "        out = self.ffn_v(z)\n",
    "        out = out + residual\n",
    "        out = self.norm_3(out)\n",
    "        out = rearrange(out,'b (c v) s -> b c v s',v=v)\n",
    "        out = out.permute(0,2,1,3)# b v c s\n",
    "        return out\n",
    "\n",
    "class ModernCNet(nn.Module):\n",
    "\n",
    "    def __init__(self,num_block,r,seq_len,channel_dim,variable_dim):\n",
    "        super(ModernCNet,self).__init__()\n",
    "        self.emb = Embedding(seq_len,channel_dim)\n",
    "        self.ModernCList = nn.ModuleList([ModernCBlock(variable_dim,r,seq_len,channel_dim) for _ in range(num_block)])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=variable_dim*seq_len*channel_dim,out_features=variable_dim*seq_len*channel_dim//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=variable_dim*seq_len*channel_dim//2,out_features=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.emb(x) # b v c s\n",
    "        #print(\"x:\",x.shape) #256 3 6 50\n",
    "        residual = x # b v c s\n",
    "        features = x\n",
    "        for block in self.ModernCList:\n",
    "            features = block(features)\n",
    "        features = features + residual\n",
    "        features = self.flatten(features) # b (v*c*s)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "\n",
    "def train(net,train_iter,val_iter,loss,num_epochs,optimizer,device=\"cuda\"):\n",
    "    net.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    val_acc_best = 1e-9\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            #print(\"X:\",X.shape)\n",
    "            y = y.to(device)\n",
    "            #print(\"y:\",y.shape) \n",
    "            y_hat = net(X)\n",
    "            #print(\"y_hat:\",y_hat.shape)\n",
    "            #y_hat = y_hat.squeeze()\n",
    "            #print(\"y_hat_squeeze:\",y_hat.shape)\n",
    "            l=loss(y_hat,y.long()).sum()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        net.eval()\n",
    "        val_acc,val_l = evaluate_accuracy(val_iter,net,loss)\n",
    "        net.train()\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        val_loss.append(val_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        val_accuracy.append(val_acc)\n",
    "        if(val_acc>val_acc_best):\n",
    "            val_acc_best = val_acc\n",
    "            train_loss_best = train_l_sum/n\n",
    "            train_acc_best = train_acc_sum/n\n",
    "            epoch_best = epoch\n",
    "            torch.save(net.state_dict(),\"weights/DCCN_BN_best.pt\")\n",
    "        print('epoch%d,loss%.4f,train acc %3f,val acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,val_acc))\n",
    "    print(\"best weight was found in epoch%d,where the train loss is %.4f,train acc is %3f,val acc is %.3f\"%(epoch_best,train_loss_best,train_acc_best,val_acc_best))\n",
    "    return train_loss,val_loss,train_accuracy,val_accuracy\n",
    "\n",
    "net = ModernCNet(3,2,100,6,3)\n",
    "net.to(device)\n",
    "optimizer=optim.Adam(net.parameters(),lr)\n",
    "\n",
    "train_loss,val_loss,train_accuracy,val_accuracy=train(net,train_loader,val_loader,loss,epochs,optimizer)\n",
    "visualization(train_loss,val_loss,train_accuracy,val_accuracy)\n",
    "net.load_state_dict(torch.load(\"weights/DCCN_BN_best.pt\"))\n",
    "test_acc,_ = evaluate_accuracy(test_loader,net,loss)\n",
    "print(\"Accuracy on test set : %.3f\"%(test_acc))\n",
    "test_precision,test_recall,test_f1_score = evaluate_PR(test_loader,net,loss)\n",
    "print(\"test precision:\",test_precision,\" test recall:\",test_recall,\" test f1_score:\",test_f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e3a75e3672151672"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3f79ff20fa5ec80d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
