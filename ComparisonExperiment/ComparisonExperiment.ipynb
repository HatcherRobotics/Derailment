{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from einops import rearrange\n",
    "import torch.nn.utils.weight_norm as weight_norm\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class my_Dataset(Data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = features\n",
    "        self.y = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "class WheelRailTrajectoryDataset:\n",
    "    def __init__(self,label=3):\n",
    "        standard_scaler = StandardScaler()\n",
    "        self.raw_data1 = pd.read_csv(\"../dataset/curve_R400.csv\")\n",
    "        self.raw_data2 = pd.read_csv(\"../dataset/curve_R1000.csv\")\n",
    "        self.raw_data3 = pd.read_csv(\"../dataset/curve_R2000.csv\")\n",
    "        self.raw_data4 = pd.read_csv(\"../dataset/curve_R3000.csv\")\n",
    "        self.raw_data5 = pd.read_csv(\"../dataset/curve_R4000.csv\")\n",
    "        self.raw_data6 = pd.read_csv(\"../dataset/curve_R5000.csv\")\n",
    "        self.raw_data = pd.concat([self.raw_data1,self.raw_data2,self.raw_data3,self.raw_data4,self.raw_data5,self.raw_data6])\n",
    "        self.label = label\n",
    "        #标准化\n",
    "        self.data = standard_scaler.fit_transform(self.raw_data)\n",
    "        self.mean = standard_scaler.mean_[label]\n",
    "        self.sd = math.sqrt(standard_scaler.var_[label])\n",
    "\n",
    "\n",
    "    def construct_set(self, train_por=0.6,val_por=0.2,test_por=0.2, window_size=100):\n",
    "        X = []\n",
    "        Y = []\n",
    "        list = [self.raw_data1.shape[0],self.raw_data2.shape[0],self.raw_data3.shape[0],self.raw_data4.shape[0],self.raw_data5.shape[0],self.raw_data6.shape[0]]\n",
    "        num = 0\n",
    "        for j in range(len(list)):\n",
    "            if j!=0:\n",
    "                num = num+list[j-1]\n",
    "            for i in range(list[j]-window_size):\n",
    "                seq = self.data[num+i:num+i+window_size+1]\n",
    "                X.append(seq[0:window_size,:self.label])\n",
    "                y = seq[window_size:window_size+1,self.label]\n",
    "                if(y>0.6 or y<-0.6):\n",
    "                    Y.append(1)\n",
    "                else:\n",
    "                    Y.append(0)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        print(X.shape)\n",
    "        print(Y.shape)\n",
    "        train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=test_por,train_size=train_por+val_por,shuffle=True)\n",
    "        train_x,val_x,train_y,val_y =  train_test_split(train_x,train_y,test_size=val_por/(val_por+train_por),train_size=train_por/(val_por+train_por),shuffle=True)\n",
    "\n",
    "        train_set = my_Dataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "        val_set = my_Dataset(torch.Tensor(val_x), torch.Tensor(val_y))\n",
    "        test_set = my_Dataset(torch.Tensor(test_x), torch.Tensor(test_y))\n",
    "        return train_set, val_set, test_set\n",
    "\n",
    "TrajectoryData = WheelRailTrajectoryDataset()\n",
    "train_set, val_set, test_set = TrajectoryData.construct_set()\n",
    "batch_size = 256\n",
    "train_loader = Data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = Data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "test_loader = Data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c0b1b9891c65176"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, loss,device=\"cuda\"):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        l = loss(y_hat, y.long()).sum()\n",
    "        test_l_sum += l.item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum/n, test_l_sum/n\n",
    "\n",
    "def visualization(train_loss,val_loss,train_accuracy,val_accuracy):\n",
    "    plt.figure(1)\n",
    "    x1=np.linspace(0,len(train_loss),len(train_loss))\n",
    "    plt.plot(x1,train_loss,label='train_loss',linewidth=1.5)\n",
    "    plt.plot(x1,val_loss,label='test_loss',linewidth=1.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(2)\n",
    "    x2 = np.linspace(0,len(train_accuracy),len(train_accuracy))\n",
    "    plt.plot(x2,train_accuracy,label='train_accuracy',linewidth=1.5)\n",
    "    plt.plot(x2,val_accuracy,label='val_accuracy',linewidth=1.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_PR(data_iter, net, loss,device=\"cuda\"):\n",
    "    n = 0\n",
    "    precision_sum,recall_sum,f1_score_sum = 0.0,0.0,0.0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y.long()).sum()\n",
    "        y_hat = y_hat.argmax(dim=1)\n",
    "        y_hat = y_hat.squeeze().cpu().detach().numpy()\n",
    "        y = y.cpu().detach().numpy()\n",
    "        precision_sum += metrics.precision_score(y,y_hat)\n",
    "        recall_sum += metrics.recall_score(y,y_hat)\n",
    "        f1_score_sum += metrics.f1_score(y,y_hat)\n",
    "        test_l_sum += l.item()\n",
    "        #n += y.shape[0]\n",
    "        n+=1\n",
    "    return precision_sum/n,recall_sum/n,f1_score_sum/n\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "lr = 0.0001\n",
    "epochs=300\n",
    "loss=nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6c0492d3b7fbd3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.DCCN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2413b55532b859b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,seq_len,channel_dim,P=1,S=1):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.emb = nn.Conv1d(in_channels=1, out_channels=channel_dim,\n",
    "                             kernel_size=P,padding=0,stride=S,groups=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #input b s v\n",
    "        b = x.shape[0]\n",
    "        x = x.permute(0,2,1) # b v s\n",
    "        x = x.unsqueeze(2)#B V 1 S\n",
    "        x = rearrange(x, 'b v c s -> (b v) c s')# B*V 1 S\n",
    "        out = self.emb(x) # b v c s\n",
    "        out = rearrange(out, '(b v) c s -> b v c s', b=b)\n",
    "        return out # b v c s\n",
    "class ModernCBlock(nn.Module):\n",
    "    def __init__(self,variable_dim,r,seq_len,channel_dim):\n",
    "        super(ModernCBlock,self).__init__()\n",
    "        #conv_s model seq\n",
    "        self.conv_b1 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//2,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b2 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//4,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b3 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//5,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "        self.conv_b4 =nn.Conv1d(in_channels=variable_dim*channel_dim,out_channels=variable_dim*channel_dim,kernel_size=seq_len//10,padding='same',stride=1,groups=variable_dim*channel_dim)\n",
    "\n",
    "        #ffn_c model channel\n",
    "        self.ffn_c = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=variable_dim*channel_dim, out_channels=r*variable_dim*channel_dim,padding='same',kernel_size=1,groups=variable_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels=r*variable_dim*channel_dim, out_channels=variable_dim*channel_dim,padding='same',kernel_size=1,groups=variable_dim)\n",
    "        )\n",
    "\n",
    "        #ffn_v model variable\n",
    "        self.ffn_v = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=variable_dim*channel_dim, out_channels=r*variable_dim*channel_dim,padding='same',kernel_size=1,groups=channel_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels=r*variable_dim*channel_dim, out_channels=variable_dim*channel_dim,padding='same',kernel_size=1,groups=channel_dim)\n",
    "        )\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(variable_dim*channel_dim)\n",
    "        self.norm_2 = nn.LayerNorm(variable_dim*channel_dim)\n",
    "        self.norm_3 = nn.LayerNorm(channel_dim*variable_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #input : b v c s\n",
    "        v = x.shape[1]\n",
    "        x = rearrange(x, 'b v c s -> b (v c) s')\n",
    "        residual = x\n",
    "        y1 = self.conv_b1(x)\n",
    "        y2 = self.conv_b2(x)\n",
    "        y3 = self.conv_b3(x)\n",
    "        y4 = self.conv_b4(x)\n",
    "        y = y1 + y2 +y3 + y4 + residual\n",
    "        y = y.permute(0,2,1)\n",
    "        y = self.norm_1(y)\n",
    "        y = y.permute(0,2,1)\n",
    "        y = rearrange(y,'b (v c) s -> b v c s', v=v)\n",
    "\n",
    "        y = rearrange(y, 'b v c s -> b (v c) s')\n",
    "        residual = y\n",
    "        z = self.ffn_c(y) # \n",
    "        z = z + residual\n",
    "        z = z.permute(0,2,1)\n",
    "        z = self.norm_2(z)\n",
    "        z = z.permute(0,2,1)\n",
    "        z = rearrange(z,'b (v c) s -> b v c s', v=v)\n",
    "        z = z.permute(0,2,1,3)\n",
    "\n",
    "        z = rearrange(z, 'b c v s -> b (c v) s')\n",
    "        residual = z\n",
    "        out = self.ffn_v(z)\n",
    "        out = out + residual\n",
    "        out = out.permute(0,2,1)\n",
    "        out = self.norm_3(out)\n",
    "        out = out.permute(0,2,1)\n",
    "        out = rearrange(out,'b (c v) s -> b c v s',v=v)\n",
    "        out = out.permute(0,2,1,3)# b v c s\n",
    "        return out\n",
    "\n",
    "class ModernCNet(nn.Module):\n",
    "\n",
    "    def __init__(self,num_block,r,seq_len,channel_dim,variable_dim):\n",
    "        super(ModernCNet,self).__init__()\n",
    "        self.emb = Embedding(seq_len,channel_dim)\n",
    "        self.ModernCList = nn.ModuleList([ModernCBlock(variable_dim,r,seq_len,channel_dim) for _ in range(num_block)])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=variable_dim*seq_len*channel_dim,out_features=variable_dim*seq_len*channel_dim//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=variable_dim*seq_len*channel_dim//2,out_features=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.emb(x) # b v c s\n",
    "        #print(\"x:\",x.shape) #256 3 6 50\n",
    "        residual = x # b v c s\n",
    "        features = x\n",
    "        for block in self.ModernCList:\n",
    "            features = block(features)\n",
    "        features = features + residual\n",
    "        features = self.flatten(features) # b (v*c*s)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "\n",
    "def train(net,train_iter,val_iter,loss,num_epochs,optimizer,device=\"cuda\"):\n",
    "    net.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    val_acc_best = 1e-9\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            #print(\"X:\",X.shape)\n",
    "            y = y.to(device)\n",
    "            #print(\"y:\",y.shape) \n",
    "            y_hat = net(X)\n",
    "            #print(\"y_hat:\",y_hat.shape)\n",
    "            #y_hat = y_hat.squeeze()\n",
    "            #print(\"y_hat_squeeze:\",y_hat.shape)\n",
    "            l=loss(y_hat,y.long()).sum()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        net.eval()\n",
    "        val_acc,val_l = evaluate_accuracy(val_iter,net,loss)\n",
    "        net.train()\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        val_loss.append(val_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        val_accuracy.append(val_acc)\n",
    "        if(val_acc>val_acc_best):\n",
    "            val_acc_best = val_acc\n",
    "            train_loss_best = train_l_sum/n\n",
    "            train_acc_best = train_acc_sum/n\n",
    "            epoch_best = epoch\n",
    "            torch.save(net.state_dict(),\"weights/DCCN_best.pt\")\n",
    "        print('epoch%d,loss%.4f,train acc %3f,val acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,val_acc))\n",
    "    print(\"best weight was found in epoch%d,where the train loss is %.4f,train acc is %3f,val acc is %.3f\"%(epoch_best,train_loss_best,train_acc_best,val_acc_best))\n",
    "    return train_loss,val_loss,train_accuracy,val_accuracy\n",
    "\n",
    "net = ModernCNet(3,2,100,6,3)\n",
    "net.to(device)\n",
    "optimizer=optim.Adam(net.parameters(),lr)\n",
    "\n",
    "train_loss,val_loss,train_accuracy,val_accuracy=train(net,train_loader,val_loader,loss,epochs,optimizer)\n",
    "visualization(train_loss,val_loss,train_accuracy,val_accuracy)\n",
    "net.load_state_dict(torch.load(\"weights/DCCN_best.pt\"))\n",
    "test_acc,_ = evaluate_accuracy(test_loader,net,loss)\n",
    "print(\"Accuracy on test set : %.3f\"%(test_acc))\n",
    "test_precision,test_recall,test_f1_score = evaluate_PR(test_loader,net,loss)\n",
    "print(\"test precision:\",test_precision,\" test recall:\",test_recall,\" test f1_score:\",test_f1_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea98e7a0339a0ba1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcab741a347f8e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self,batch_size,input_features,hidden_size,num_layers,device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_features,hidden_size=hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size,out_features=hidden_size//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=hidden_size//2,out_features=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "    def forward(self,x):\n",
    "        #print(\"x\",x.shape)\n",
    "        h,c = (torch.zeros(self.num_layers,self.batch_size,self.hidden_size).to(x.device) for _ in range(2))\n",
    "        H,(h,c) = self.lstm(x,(h,c))\n",
    "        out = H[:,-1,:].squeeze()\n",
    "        out = self.classifier(out)\n",
    "        out = out.squeeze()\n",
    "        return out\n",
    "\n",
    "def train(net,train_iter,val_iter,loss,num_epochs,optimizer,device=\"cuda\"):\n",
    "    net.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    val_acc_best = 1e-9\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            #print(\"X:\",X.shape)\n",
    "            y = y.to(device)\n",
    "            #print(\"y:\",y.shape) \n",
    "            y_hat = net(X)\n",
    "            #print(\"y_hat:\",y_hat.shape)\n",
    "            #y_hat = y_hat.squeeze()\n",
    "            #print(\"y_hat_squeeze:\",y_hat.shape)\n",
    "            l=loss(y_hat,y.long()).sum()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        net.eval()\n",
    "        val_acc,val_l = evaluate_accuracy(val_iter,net,loss)\n",
    "        net.train()\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        val_loss.append(val_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        val_accuracy.append(val_acc)\n",
    "        if(val_acc>val_acc_best):\n",
    "            val_acc_best = val_acc\n",
    "            train_loss_best = train_l_sum/n\n",
    "            train_acc_best = train_acc_sum/n\n",
    "            epoch_best = epoch\n",
    "            torch.save(net.state_dict(),\"weights/LSTM_best.pt\")\n",
    "        print('epoch%d,loss%.4f,train acc %3f,val acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,val_acc))\n",
    "    print(\"best weight was found in epoch%d,where the train loss is %.4f,train acc is %3f,val acc is %.3f\"%(epoch_best,train_loss_best,train_acc_best,val_acc_best))\n",
    "    return train_loss,val_loss,train_accuracy,val_accuracy\n",
    "net = LSTMNet(batch_size=batch_size,input_features=train_set.X.shape[-1],hidden_size=256,num_layers=1)\n",
    "net.to(device)\n",
    "optimizer=optim.Adam(net.parameters(),lr)\n",
    "train_loss,val_loss,train_accuracy,val_accuracy=train(net,train_loader,val_loader,loss,epochs,optimizer)\n",
    "visualization(train_loss,val_loss,train_accuracy,val_accuracy)\n",
    "net.load_state_dict(torch.load(\"weights/LSTM_best.pt\"))\n",
    "test_acc,_ = evaluate_accuracy(test_loader,net,loss)\n",
    "print(\"Accuracy on test set : %.3f\"%(test_acc))\n",
    "test_precision,test_recall,test_f1_score = evaluate_PR(test_loader,net,loss)\n",
    "print(\"test precision:\",test_precision,\" test recall:\",test_recall,\" test f1_score:\",test_f1_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0911d882fa3cae6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.TCN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9106fbfddd830d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self,chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = int(chomp_size)\n",
    "    def forward(self,x):\n",
    "        return x[:,:,0:-self.chomp_size].contiguous()\n",
    "\n",
    "class TCNResidualBlock(nn.Module):\n",
    "    def __init__(self,inchannel,outchannel,kernelsize,dilation):\n",
    "        super(TCNResidualBlock,self).__init__()\n",
    "        self.ke = int(kernelsize+(kernelsize-1)*(dilation-1))\n",
    "        self.conv = nn.Sequential(\n",
    "            weight_norm(nn.Conv1d(inchannel,outchannel,kernel_size=kernelsize,dilation=dilation,padding=self.ke-1)),\n",
    "            Chomp1d(self.ke-1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            weight_norm(nn.Conv1d(outchannel,outchannel,kernel_size=kernelsize,dilation=dilation,padding=self.ke-1)),\n",
    "            Chomp1d(self.ke-1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            weight_norm(nn.Conv1d(inchannel,outchannel,kernel_size=1)),\n",
    "        )\n",
    "\n",
    "    def forward(self,X):\n",
    "        Y = self.conv(X)\n",
    "        #print(\"Y:\",Y.shape)\n",
    "        X = self.conv1x1(X)\n",
    "        #print(\"X:\",X.shape)\n",
    "        out = F.relu(X+Y)\n",
    "        return out\n",
    "class TCNet(nn.Module):\n",
    "    def __init__(self,seq_len,inchannels,outchannels,kernel_size,dilations):\n",
    "        super(TCNet,self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            TCNResidualBlock(inchannel=inchannels[0],outchannel=outchannels[0],kernelsize=kernel_size[0],dilation=dilations[0]),\n",
    "            TCNResidualBlock(inchannel=inchannels[1],outchannel=outchannels[1],kernelsize=kernel_size[1],dilation=dilations[1]),\n",
    "            TCNResidualBlock(inchannel=inchannels[2],outchannel=outchannels[2],kernelsize=kernel_size[2],dilation=dilations[2])\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=3*seq_len,out_features=3*seq_len//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=3*seq_len//2,out_features=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "    def forward(self,X):\n",
    "        #in_channels代表输入序列的特征维数\n",
    "        #数据集输入为batch_size * seq_length * features\n",
    "        #Conv1d要求输入为batch_size*features*seq_length\n",
    "        X = X.permute(0,2,1).contiguous()\n",
    "        #print(\"X:\",X.shape)\n",
    "        features = self.features(X)\n",
    "        #batch_size*features(output_features)*seq_length 类似LSTM的H矩阵\n",
    "        out = self.flatten(features)\n",
    "        #batch_size*features*pred_len\n",
    "        out = self.classifier(out)\n",
    "        #batch_size*pred_len*features\n",
    "        return out\n",
    "def train(net,train_iter,val_iter,loss,num_epochs,optimizer,device=\"cuda\"):\n",
    "    net.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    val_acc_best = 1e-9\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            #print(\"X:\",X.shape)\n",
    "            y = y.to(device)\n",
    "            #print(\"y:\",y.shape) \n",
    "            y_hat = net(X)\n",
    "            #print(\"y_hat:\",y_hat.shape)\n",
    "            #y_hat = y_hat.squeeze()\n",
    "            #print(\"y_hat_squeeze:\",y_hat.shape)\n",
    "            l=loss(y_hat,y.long()).sum()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        net.eval()\n",
    "        val_acc,val_l = evaluate_accuracy(val_iter,net,loss)\n",
    "        net.train()\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        val_loss.append(val_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        val_accuracy.append(val_acc)\n",
    "        if(val_acc>val_acc_best):\n",
    "            val_acc_best = val_acc\n",
    "            train_loss_best = train_l_sum/n\n",
    "            train_acc_best = train_acc_sum/n\n",
    "            epoch_best = epoch\n",
    "            torch.save(net.state_dict(),\"weights/TCN_best.pt\")\n",
    "        print('epoch%d,loss%.4f,train acc %3f,val acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,val_acc))\n",
    "    print(\"best weight was found in epoch%d,where the train loss is %.4f,train acc is %3f,val acc is %.3f\"%(epoch_best,train_loss_best,train_acc_best,val_acc_best))\n",
    "    return train_loss,val_loss,train_accuracy,val_accuracy\n",
    "net = TCNet(seq_len=100,inchannels=[train_set.X.shape[-1],train_set.X.shape[-1]*2,train_set.X.shape[-1]*4],outchannels=[train_set.X.shape[-1]*2,train_set.X.shape[-1]*4,train_set.X.shape[-1]],kernel_size=[7,5,3],dilations=[1,2,4])\n",
    "net.to(device)\n",
    "optimizer=optim.Adam(net.parameters(),lr)\n",
    "train_loss,val_loss,train_accuracy,val_accuracy=train(net,train_loader,val_loader,loss,epochs,optimizer)\n",
    "visualization(train_loss,val_loss,train_accuracy,val_accuracy)\n",
    "net.load_state_dict(torch.load(\"weights/TCN_best.pt\"))\n",
    "test_acc,_ = evaluate_accuracy(test_loader,net,loss)\n",
    "print(\"Accuracy on test set : %.3f\"%(test_acc))\n",
    "test_precision,test_recall,test_f1_score = evaluate_PR(test_loader,net,loss)\n",
    "print(\"test precision:\",test_precision,\" test recall:\",test_recall,\" test f1_score:\",test_f1_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d82d15740c402fb7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.SCINet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aba4638b5e7141e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Splitting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Splitting, self).__init__()\n",
    "\n",
    "    def even(self, x):\n",
    "        return x[:, ::2, :]\n",
    "\n",
    "    def odd(self, x):\n",
    "        return x[:, 1::2, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Returns the odd and even part'''\n",
    "        return (self.even(x), self.odd(x))\n",
    "\n",
    "class Interactor(nn.Module):\n",
    "    # in_planes input_dim\n",
    "    def __init__(self, in_planes, splitting=True,\n",
    "                 kernel = 5, dropout=0.5, groups = 1, hidden_size = 1, INN = True):\n",
    "        super(Interactor, self).__init__()\n",
    "        self.modified = INN\n",
    "        self.kernel_size = kernel\n",
    "        self.dilation = 1\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.groups = groups\n",
    "        #输入输出长度等长\n",
    "        if self.kernel_size % 2 == 0:\n",
    "            pad_l = self.dilation * (self.kernel_size - 2) // 2 + 1 #by default: stride==1 \n",
    "            pad_r = self.dilation * (self.kernel_size) // 2 + 1 #by default: stride==1 \n",
    "\n",
    "        else:\n",
    "            pad_l = self.dilation * (self.kernel_size - 1) // 2 + 1 # we fix the kernel size of the second layer as 3.\n",
    "            pad_r = self.dilation * (self.kernel_size - 1) // 2 + 1\n",
    "        self.splitting = splitting\n",
    "        self.split = Splitting()\n",
    "\n",
    "        modules_P = []\n",
    "        modules_U = []\n",
    "        modules_psi = []\n",
    "        modules_phi = []\n",
    "        prev_size = 1\n",
    "\n",
    "        size_hidden = self.hidden_size\n",
    "        modules_P += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            #input_dim == in_planes == channels\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_U += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        modules_phi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        modules_psi += [\n",
    "            nn.ReplicationPad1d((pad_l, pad_r)),\n",
    "            nn.Conv1d(in_planes * prev_size, int(in_planes * size_hidden),\n",
    "                      kernel_size=self.kernel_size, dilation=self.dilation, stride=1, groups= self.groups),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv1d(int(in_planes * size_hidden), in_planes,\n",
    "                      kernel_size=3, stride=1, groups= self.groups),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.phi = nn.Sequential(*modules_phi)\n",
    "        self.psi = nn.Sequential(*modules_psi)\n",
    "        self.P = nn.Sequential(*modules_P)\n",
    "        self.U = nn.Sequential(*modules_U)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #输入 batch_size seq_len feature_dim\n",
    "        if self.splitting:\n",
    "            (x_even, x_odd) = self.split(x)\n",
    "        else:\n",
    "            (x_even, x_odd) = x\n",
    "\n",
    "        if self.modified:\n",
    "            x_even = x_even.permute(0, 2, 1)# B L D\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd.mul(torch.exp(self.phi(x_even)))\n",
    "            c = x_even.mul(torch.exp(self.psi(x_odd)))\n",
    "\n",
    "            x_even_update = c + self.U(d)\n",
    "            x_odd_update = d - self.P(c)\n",
    "\n",
    "            return (x_even_update, x_odd_update) # B D L 与 Conv1d的输出 N C L对应\n",
    "\n",
    "        else:\n",
    "            x_even = x_even.permute(0, 2, 1)\n",
    "            x_odd = x_odd.permute(0, 2, 1)\n",
    "\n",
    "            d = x_odd - self.P(x_even)\n",
    "            c = x_even + self.U(d)\n",
    "\n",
    "            return (c, d)\n",
    "\n",
    "class InteractorLevel(nn.Module):\n",
    "    def __init__(self, in_planes, kernel, dropout, groups , hidden_size, INN):\n",
    "        super(InteractorLevel, self).__init__()\n",
    "        self.level = Interactor(in_planes = in_planes, splitting=True,\n",
    "                                kernel = kernel, dropout=dropout, groups = groups, hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.level(x)\n",
    "        return (x_even_update, x_odd_update) # B D L\n",
    "\n",
    "class LevelSCINet(nn.Module):\n",
    "    def __init__(self,in_planes, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super(LevelSCINet, self).__init__()\n",
    "        self.interact = InteractorLevel(in_planes= in_planes, kernel = kernel_size, dropout = dropout, groups =groups , hidden_size = hidden_size, INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x_even_update, x_odd_update) = self.interact(x) # B D L\n",
    "        return x_even_update.permute(0, 2, 1), x_odd_update.permute(0, 2, 1) #even: B, T, D odd: B, T, D\n",
    "\n",
    "class SCINet_Tree(nn.Module):\n",
    "    '''\n",
    "    递归 二叉树的格式\n",
    "    '''\n",
    "    def __init__(self, in_planes, current_level, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.current_level = current_level\n",
    "\n",
    "\n",
    "        self.workingblock = LevelSCINet(\n",
    "            in_planes = in_planes,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout = dropout,\n",
    "            groups= groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "\n",
    "\n",
    "        if current_level!=0:\n",
    "            self.SCINet_Tree_odd=SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "            self.SCINet_Tree_even=SCINet_Tree(in_planes, current_level-1, kernel_size, dropout, groups, hidden_size, INN)\n",
    "\n",
    "    def zip_up_the_pants(self, even, odd):\n",
    "        even = even.permute(1, 0, 2)\n",
    "        odd = odd.permute(1, 0, 2) #L, B, D\n",
    "        #把序列长度维度调到第0维再拼接\n",
    "        even_len = even.shape[0]\n",
    "        odd_len = odd.shape[0]\n",
    "        mlen = min((odd_len, even_len))\n",
    "        _ = []\n",
    "        for i in range(mlen): #偶 奇 偶 ...\n",
    "            _.append(even[i].unsqueeze(0))\n",
    "            _.append(odd[i].unsqueeze(0))\n",
    "        if odd_len < even_len: #奇数序列<偶数序列 0 1 2\n",
    "            _.append(even[-1].unsqueeze(0))\n",
    "        return torch.cat(_,0).permute(1,0,2) #B, L, D        #Conv1d input N C L\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_even_update, x_odd_update= self.workingblock(x)\n",
    "        # We recursively reordered these sub-series. You can run the ./utils/recursive_demo.py to emulate this procedure. \n",
    "        if self.current_level ==0:\n",
    "            return self.zip_up_the_pants(x_even_update, x_odd_update)#输出一条最终序列\n",
    "        else:\n",
    "            return self.zip_up_the_pants(self.SCINet_Tree_even(x_even_update), self.SCINet_Tree_odd(x_odd_update))\n",
    "\n",
    "class EncoderTree(nn.Module):\n",
    "    def __init__(self, in_planes,  num_levels, kernel_size, dropout, groups, hidden_size, INN):\n",
    "        super().__init__()\n",
    "        self.levels=num_levels\n",
    "        self.SCINet_Tree = SCINet_Tree(\n",
    "            in_planes = in_planes,\n",
    "            current_level = num_levels-1,\n",
    "            kernel_size = kernel_size,\n",
    "            dropout =dropout ,\n",
    "            groups = groups,\n",
    "            hidden_size = hidden_size,\n",
    "            INN = INN)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x= self.SCINet_Tree(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SCICNet(nn.Module):\n",
    "    def __init__(self, n_classes=2, input_len=100, input_dim = 3, hid_size = 1,\n",
    "                 num_levels = 2, groups = 1, kernel = 5, dropout = 0.5):\n",
    "        super(SCICNet, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = n_classes\n",
    "        self.input_dim = input_dim\n",
    "        self.num_levels = num_levels\n",
    "        self.kernel_size = kernel\n",
    "        self.dropout = dropout\n",
    "        self.groups = groups\n",
    "        self.hidden_size = hid_size\n",
    "\n",
    "        self.features = EncoderTree(\n",
    "            in_planes=self.input_dim,\n",
    "            num_levels = self.num_levels,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout,\n",
    "            groups = self.groups,\n",
    "            hidden_size = self.hidden_size,\n",
    "            INN = True)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_len*input_dim,input_len*input_dim//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(input_len*input_dim//2,n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"x:\",x.shape)\n",
    "        #输入B L D\n",
    "        assert self.input_len % (np.power(2, self.num_levels)) == 0 # evenly divided the input length into two parts. (e.g., 32 -> 16 -> 8 -> 4 for 3 levels)\n",
    "        y = self.features(x)+x #B L D 32 96 3\n",
    "        y = self.flatten(y)\n",
    "        out = self.classifier(y)\n",
    "        return out\n",
    "\n",
    "net = SCICNet()\n",
    "net.to(device)\n",
    "optimizer=optim.Adam(net.parameters(),lr)\n",
    "def train(net,train_iter,val_iter,loss,num_epochs,optimizer,device=\"cuda\"):\n",
    "    net.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    val_acc_best = 1e-9\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            #print(\"X:\",X.shape)\n",
    "            y = y.to(device)\n",
    "            #print(\"y:\",y.shape) \n",
    "            y_hat = net(X)\n",
    "            #print(\"y_hat:\",y_hat.shape)\n",
    "            #y_hat = y_hat.squeeze()\n",
    "            #print(\"y_hat_squeeze:\",y_hat.shape)\n",
    "            l=loss(y_hat,y.long()).sum()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        net.eval()\n",
    "        val_acc,val_l = evaluate_accuracy(val_iter,net,loss)\n",
    "        net.train()\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        val_loss.append(val_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        val_accuracy.append(val_acc)\n",
    "        if(val_acc>val_acc_best):\n",
    "            val_acc_best = val_acc\n",
    "            train_loss_best = train_l_sum/n\n",
    "            train_acc_best = train_acc_sum/n\n",
    "            epoch_best = epoch\n",
    "            torch.save(net.state_dict(),\"weights/SCINet_best.pt\")\n",
    "        print('epoch%d,loss%.4f,train acc %3f,val acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,val_acc))\n",
    "    print(\"best weight was found in epoch%d,where the train loss is %.4f,train acc is %3f,val acc is %.3f\"%(epoch_best,train_loss_best,train_acc_best,val_acc_best))\n",
    "    return train_loss,val_loss,train_accuracy,val_accuracy\n",
    "train_loss,val_loss,train_accuracy,val_accuracy=train(net,train_loader,val_loader,loss,epochs,optimizer)\n",
    "visualization(train_loss,val_loss,train_accuracy,val_accuracy)\n",
    "net.load_state_dict(torch.load(\"weights/SCINet_best.pt\"))\n",
    "test_acc,_ = evaluate_accuracy(test_loader,net,loss)\n",
    "print(\"Accuracy on test set : %.3f\"%(test_acc))\n",
    "test_precision,test_recall,test_f1_score = evaluate_PR(test_loader,net,loss)\n",
    "print(\"test precision:\",test_precision,\" test recall:\",test_recall,\" test f1_score:\",test_f1_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfa33d86a7d5f00d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5.NLinear"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c91d7d38b9ad9887"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e0acccb275808414"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
