{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:32:36.737725500Z",
     "start_time": "2023-12-13T12:32:36.737220100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227406, 100, 3)\n",
      "(227406,)\n"
     ]
    }
   ],
   "source": [
    "class my_Dataset(Data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = features\n",
    "        self.y = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "class WheelRailTrajectoryDataset:\n",
    "    def __init__(self,label=3):\n",
    "        standard_scaler = StandardScaler()\n",
    "        self.raw_data1 = pd.read_csv(\"../dataset/curve_R400.csv\")\n",
    "        self.raw_data2 = pd.read_csv(\"../dataset/curve_R1000.csv\")\n",
    "        self.raw_data3 = pd.read_csv(\"../dataset/curve_R2000.csv\")\n",
    "        self.raw_data4 = pd.read_csv(\"../dataset/curve_R3000.csv\")\n",
    "        self.raw_data5 = pd.read_csv(\"../dataset/curve_R4000.csv\")\n",
    "        self.raw_data6 = pd.read_csv(\"../dataset/curve_R5000.csv\")\n",
    "        self.raw_data = pd.concat([self.raw_data1,self.raw_data2,self.raw_data3,self.raw_data4,self.raw_data5,self.raw_data6])\n",
    "        self.label = label\n",
    "        #标准化\n",
    "        self.data = standard_scaler.fit_transform(self.raw_data)\n",
    "        self.mean = standard_scaler.mean_[label]\n",
    "        self.sd = math.sqrt(standard_scaler.var_[label])\n",
    "\n",
    "\n",
    "    def construct_set(self, train_por=0.6,val_por=0.2,test_por=0.2, window_size=100):\n",
    "        X = []\n",
    "        Y = []\n",
    "        list = [self.raw_data1.shape[0],self.raw_data2.shape[0],self.raw_data3.shape[0],self.raw_data4.shape[0],self.raw_data5.shape[0],self.raw_data6.shape[0]]\n",
    "        num = 0\n",
    "        for j in range(len(list)):\n",
    "            if j!=0:\n",
    "                num = num+list[j-1]\n",
    "            for i in range(list[j]-window_size):\n",
    "                seq = self.data[num+i:num+i+window_size+1]\n",
    "                X.append(seq[0:window_size,:self.label])\n",
    "                y = seq[window_size:window_size+1,self.label]\n",
    "                if(y>0.6 or y<-0.6):\n",
    "                    Y.append(1)\n",
    "                else:\n",
    "                    Y.append(0)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        print(X.shape)\n",
    "        print(Y.shape)\n",
    "        train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=test_por,train_size=train_por+val_por,shuffle=True)\n",
    "        train_x,val_x,train_y,val_y =  train_test_split(train_x,train_y,test_size=val_por/(val_por+train_por),train_size=train_por/(val_por+train_por),shuffle=True)\n",
    "\n",
    "        train_set = my_Dataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "        val_set = my_Dataset(torch.Tensor(val_x), torch.Tensor(val_y))\n",
    "        test_set = my_Dataset(torch.Tensor(test_x), torch.Tensor(test_y))\n",
    "        return train_set, val_set, test_set\n",
    "\n",
    "TrajectoryData = WheelRailTrajectoryDataset()\n",
    "train_set, val_set, test_set = TrajectoryData.construct_set()\n",
    "batch_size = 256\n",
    "train_loader = Data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = Data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "test_loader = Data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:33:28.651460100Z",
     "start_time": "2023-12-13T12:33:26.952721900Z"
    }
   },
   "id": "951d95c20ea51e49"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model,seq_len = 100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(seq_len, d_model+1)\n",
    "        #维数 seq_len X d_model\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1) #  维数 seq_lenX1\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        #维数 dmodel/2\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        #维数 seq_len X dmodel/2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        #维数 1Xseq_lenXd_model\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x batch_size X seq_len X num_features\n",
    "        #print(\"x:\",x.shape)\n",
    "        #print(\"pe:\",self.pe.shape)\n",
    "        x = x + Variable(self.pe[:,:,:x.shape[-1]], requires_grad=False)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:33:28.659975700Z",
     "start_time": "2023-12-13T12:33:28.651460100Z"
    }
   },
   "id": "52f38d833ccb112e"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class IsometricConvBlock(nn.Module):\n",
    "    def __init__(self,input_dim,mlp_dim,input_length,dilation):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=mlp_dim,kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels=mlp_dim, out_channels=input_dim,kernel_size=1),\n",
    "        )\n",
    "        self.norm_1 = nn.LayerNorm(input_dim)\n",
    "        self.norm_2 = nn.LayerNorm(input_dim)\n",
    "        #Conv1d要求输入为batch_size*features*seq_length\n",
    "        self.conv = nn.Conv1d(in_channels=input_dim, out_channels=input_dim,\n",
    "                              kernel_size=input_length+50*(dilation-1), padding=0, stride=1, groups=3,dilation=dilation)\n",
    "        self.dilation = dilation\n",
    "    def isometric_conv(self,x,dilation):\n",
    "        #batch_size feature_size seq_len\n",
    "        zeros = torch.zeros((x.shape[0], x.shape[1], x.shape[2]-1+50*(dilation-1)),device='cuda')\n",
    "        print(\"zeros:\",zeros.shape)\n",
    "        print(\"x:\",x.shape)\n",
    "        x = torch.cat((zeros, x), dim=-1)\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "    def forward(self,x):\n",
    "        #input : batch_size seq_len feature_size\n",
    "        residual = x\n",
    "        x  = x.permute(0,2,1)\n",
    "        #batch_size feature_size seq_len\n",
    "        x = self.isometric_conv(x,dilation=self.dilation)\n",
    "        x = x.permute(0,2,1)\n",
    "        print(\"x\",x.shape)\n",
    "        x = self.norm_1(x + residual)\n",
    "        residual = x\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.ffn(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        out = self.norm_2(x + residual)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:59:32.343992300Z",
     "start_time": "2023-12-13T13:59:32.342870500Z"
    }
   },
   "id": "f75fc9ec5511b68"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "class DICNet(nn.Module):\n",
    "    def __init__(self,feature_dim,mlp_dim,seq_len,dilation):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEncoding(3)\n",
    "        self.block = nn.Sequential(\n",
    "            IsometricConvBlock(feature_dim,mlp_dim,seq_len,dilation[0]),\n",
    "            IsometricConvBlock(feature_dim,mlp_dim,seq_len,dilation[1]),\n",
    "            IsometricConvBlock(feature_dim,mlp_dim,seq_len,dilation[2]),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=seq_len,out_features=seq_len//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=seq_len//2,out_features=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.compression = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=feature_dim,out_channels=mlp_dim,kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels=mlp_dim,out_channels=1,kernel_size=1)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x+self.pe(x)\n",
    "        residual = x\n",
    "        feature = self.block(x)\n",
    "        feature = self.norm(feature+residual)\n",
    "        feature = self.compression(feature).squeeze()\n",
    "        y = self.classifier(feature)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:59:32.693343600Z",
     "start_time": "2023-12-13T13:59:32.678603100Z"
    }
   },
   "id": "e1b4ad49883c301"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, loss,device=\"cuda\"):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        l = loss(y_hat, y.long()).sum()\n",
    "        test_l_sum += l.item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum/n, test_l_sum/n\n",
    "\n",
    "def train(net,train_iter,val_iter,loss,num_epochs,optimizer,device=\"cuda\"):\n",
    "    net.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    val_acc_best = 1e-9\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            #print(\"X:\",X.shape)\n",
    "            y = y.to(device)\n",
    "            #print(\"y:\",y.shape) \n",
    "            y_hat = net(X)\n",
    "            #print(\"y_hat:\",y_hat.shape)\n",
    "            #y_hat = y_hat.squeeze()\n",
    "            #print(\"y_hat_squeeze:\",y_hat.shape)\n",
    "            l=loss(y_hat,y.long()).sum()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        net.eval()\n",
    "        val_acc,val_l = evaluate_accuracy(val_iter,net,loss)\n",
    "        net.train()\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        val_loss.append(val_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        val_accuracy.append(val_acc)\n",
    "        if(val_acc>val_acc_best):\n",
    "            val_acc_best = val_acc\n",
    "            train_loss_best = train_l_sum/n\n",
    "            train_acc_best = train_acc_sum/n\n",
    "            epoch_best = epoch\n",
    "            torch.save(net.state_dict(),\"../weights/DICNet_best.pt\")\n",
    "        print('epoch%d,loss%.4f,train acc %3f,val acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,val_acc))\n",
    "    print(\"best weight was found in epoch%d,where the train loss is %.4f,train acc is %3f,val acc is %.3f\"%(epoch_best,train_loss_best,train_acc_best,val_acc_best))\n",
    "    return train_loss,val_loss,train_accuracy,val_accuracy\n",
    "\n",
    "\n",
    "def visualization(train_loss,val_loss,train_accuracy,val_accuracy):\n",
    "    plt.figure(1)\n",
    "    x1=np.linspace(0,len(train_loss),len(train_loss))\n",
    "    plt.plot(x1,train_loss,label='train_loss',linewidth=1.5)\n",
    "    plt.plot(x1,val_loss,label='test_loss',linewidth=1.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(2)\n",
    "    x2 = np.linspace(0,len(train_accuracy),len(train_accuracy))\n",
    "    plt.plot(x2,train_accuracy,label='train_accuracy',linewidth=1.5)\n",
    "    plt.plot(x2,val_accuracy,label='val_accuracy',linewidth=1.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:59:33.084564200Z",
     "start_time": "2023-12-13T13:59:33.068523500Z"
    }
   },
   "id": "7609197009daacb1"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "lr = 0.001\n",
    "epochs=150\n",
    "loss=nn.CrossEntropyLoss()\n",
    "net = DICNet(3,32,100,[1,2,4])\n",
    "net.to(device)\n",
    "optimizer=optim.Adam(net.parameters(),lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:59:33.900675Z",
     "start_time": "2023-12-13T13:59:33.890980600Z"
    }
   },
   "id": "11e3c8121f673b62"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros: torch.Size([256, 3, 99])\n",
      "x: torch.Size([256, 3, 100])\n",
      "x torch.Size([256, 100, 3])\n",
      "zeros: torch.Size([256, 3, 149])\n",
      "x: torch.Size([256, 3, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (249). Kernel size: (299). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[101], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_loss,val_loss,train_accuracy,val_accuracy\u001B[38;5;241m=\u001B[39mtrain(net,train_loader,val_loader,loss,epochs,optimizer)\n",
      "Cell \u001B[1;32mIn[99], line 29\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(net, train_iter, val_iter, loss, num_epochs, optimizer, device)\u001B[0m\n\u001B[0;32m     27\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m#print(\"y:\",y.shape) \u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m net(X)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m#print(\"y_hat:\",y_hat.shape)\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m#y_hat = y_hat.squeeze()\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m#print(\"y_hat_squeeze:\",y_hat.shape)\u001B[39;00m\n\u001B[0;32m     33\u001B[0m l\u001B[38;5;241m=\u001B[39mloss(y_hat,y\u001B[38;5;241m.\u001B[39mlong())\u001B[38;5;241m.\u001B[39msum()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[98], line 26\u001B[0m, in \u001B[0;36mDICNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     24\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpe(x)\n\u001B[0;32m     25\u001B[0m residual \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m---> 26\u001B[0m feature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock(x)\n\u001B[0;32m     27\u001B[0m feature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(feature\u001B[38;5;241m+\u001B[39mresidual)\n\u001B[0;32m     28\u001B[0m feature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression(feature)\u001B[38;5;241m.\u001B[39msqueeze()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[97], line 29\u001B[0m, in \u001B[0;36mIsometricConvBlock.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     27\u001B[0m x  \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m#batch_size feature_size seq_len\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39misometric_conv(x,dilation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation)\n\u001B[0;32m     30\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m,x\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[1;32mIn[97], line 21\u001B[0m, in \u001B[0;36mIsometricConvBlock.isometric_conv\u001B[1;34m(self, x, dilation)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx:\u001B[39m\u001B[38;5;124m\"\u001B[39m,x\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     20\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((zeros, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv(x)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:310\u001B[0m, in \u001B[0;36mConv1d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:306\u001B[0m, in \u001B[0;36mConv1d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    304\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    305\u001B[0m                     _single(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 306\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(\u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    307\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Calculated padded input size per channel: (249). Kernel size: (299). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "train_loss,val_loss,train_accuracy,val_accuracy=train(net,train_loader,val_loader,loss,epochs,optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T13:59:34.307018200Z",
     "start_time": "2023-12-13T13:59:34.206517400Z"
    }
   },
   "id": "e5654f0acf048791"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualization(train_loss,val_loss,train_accuracy,val_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ad2d2f3f9bb28f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"../weights/DICNet_best.pt\"))\n",
    "test_acc,_ = evaluate_accuracy(test_loader,net,loss)\n",
    "print(\"在测试集上的准确率为:%.3f\"%(test_acc))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c87ea5dbf682b22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_PR(data_iter, net, loss,device=\"cuda\"):\n",
    "    n = 0\n",
    "    precision_sum,recall_sum,f1_score_sum = 0.0,0.0,0.0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y.long()).sum()\n",
    "        y_hat = y_hat.argmax(dim=1)\n",
    "        y_hat = y_hat.squeeze().cpu().detach().numpy()\n",
    "        y = y.cpu().detach().numpy()\n",
    "        precision_sum += metrics.precision_score(y,y_hat)\n",
    "        recall_sum += metrics.recall_score(y,y_hat)\n",
    "        f1_score_sum += metrics.f1_score(y,y_hat)\n",
    "        test_l_sum += l.item()\n",
    "        #n += y.shape[0]\n",
    "        n+=1\n",
    "    return precision_sum/n,recall_sum/n,f1_score_sum/n\n",
    "test_precision,test_recall,test_f1_score = evaluate_PR(test_loader,net,loss)\n",
    "print(\"test precision:\",test_precision,\" test recall:\",test_recall,\" test f1_score:\",test_f1_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6624246d9aab0bc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
