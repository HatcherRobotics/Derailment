{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:25:25.438195600Z",
     "start_time": "2023-12-13T07:25:23.049603300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227406, 100, 3)\n",
      "(227406,)\n"
     ]
    }
   ],
   "source": [
    "class my_Dataset(Data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = features\n",
    "        self.y = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "class WheelRailTrajectoryDataset:\n",
    "    def __init__(self,label=3):\n",
    "        standard_scaler = StandardScaler()\n",
    "        self.raw_data1 = pd.read_csv(\"../dataset/curve_R400.csv\")\n",
    "        self.raw_data2 = pd.read_csv(\"../dataset/curve_R1000.csv\")\n",
    "        self.raw_data3 = pd.read_csv(\"../dataset/curve_R2000.csv\")\n",
    "        self.raw_data4 = pd.read_csv(\"../dataset/curve_R3000.csv\")\n",
    "        self.raw_data5 = pd.read_csv(\"../dataset/curve_R4000.csv\")\n",
    "        self.raw_data6 = pd.read_csv(\"../dataset/curve_R5000.csv\")\n",
    "        self.raw_data = pd.concat([self.raw_data1,self.raw_data2,self.raw_data3,self.raw_data4,self.raw_data5,self.raw_data6])\n",
    "        self.label = label\n",
    "        #标准化\n",
    "        self.data = standard_scaler.fit_transform(self.raw_data)\n",
    "        self.mean = standard_scaler.mean_[label]\n",
    "        self.sd = math.sqrt(standard_scaler.var_[label])\n",
    "\n",
    "\n",
    "    def construct_set(self, train_por=0.6,val_por=0.2,test_por=0.2, window_size=100):\n",
    "        X = []\n",
    "        Y = []\n",
    "        list = [self.raw_data1.shape[0],self.raw_data2.shape[0],self.raw_data3.shape[0],self.raw_data4.shape[0],self.raw_data5.shape[0],self.raw_data6.shape[0]]\n",
    "        num = 0\n",
    "        for j in range(len(list)):\n",
    "            if j!=0:\n",
    "                num = num+list[j-1]\n",
    "            for i in range(list[j]-window_size):\n",
    "                seq = self.data[num+i:num+i+window_size+1]\n",
    "                X.append(seq[0:window_size,:self.label])\n",
    "                y = seq[window_size:window_size+1,self.label]\n",
    "                if(y>0.6 or y<-0.6):\n",
    "                    Y.append(1)\n",
    "                else:\n",
    "                    Y.append(0)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        print(X.shape)\n",
    "        print(Y.shape)\n",
    "        train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=test_por,train_size=train_por+val_por,shuffle=True)\n",
    "        train_x,val_x,train_y,val_y =  train_test_split(train_x,train_y,test_size=val_por/(val_por+train_por),train_size=train_por/(val_por+train_por),shuffle=True)\n",
    "\n",
    "        train_set = my_Dataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "        val_set = my_Dataset(torch.Tensor(val_x), torch.Tensor(val_y))\n",
    "        test_set = my_Dataset(torch.Tensor(test_x), torch.Tensor(test_y))\n",
    "        return train_set, val_set, test_set\n",
    "\n",
    "TrajectoryData = WheelRailTrajectoryDataset()\n",
    "train_set, val_set, test_set = TrajectoryData.construct_set()\n",
    "batch_size = 256\n",
    "train_loader = Data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = Data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "test_loader = Data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:25:58.351709700Z",
     "start_time": "2023-12-13T07:25:57.343203800Z"
    }
   },
   "id": "9f12896218464d0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model,seq_len = 100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(seq_len, d_model+1)\n",
    "        #维数 seq_len X d_model\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1) #  维数 seq_lenX1\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        #维数 dmodel/2\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        #维数 seq_len X dmodel/2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        #维数 1Xseq_lenXd_model\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x batch_size X seq_len X num_features\n",
    "        #print(\"x:\",x.shape)\n",
    "        #print(\"pe:\",self.pe.shape)\n",
    "        x = x + Variable(self.pe[:,:,:x.shape[-1]], requires_grad=False)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:25:59.092733900Z",
     "start_time": "2023-12-13T07:25:59.089187Z"
    }
   },
   "id": "7260dee5029d1565"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class FourierBlock(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, input_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm_1 = nn.LayerNorm(input_dim)\n",
    "        self.norm_2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def fourier_transform(self, x):\n",
    "        return torch.fft.fft(x, dim=-1).real\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.fourier_transform(x)\n",
    "        x = self.norm_1(x + residual)\n",
    "        residual = x\n",
    "        x = self.mlp(x)\n",
    "        out = self.norm_2(x + residual)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:25:59.850124400Z",
     "start_time": "2023-12-13T07:25:59.818347900Z"
    }
   },
   "id": "25e5ac41a5a6911e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class FNet(nn.Module):\n",
    "    def __init__(self,num_f,feature_dim,mlp_dim,seq_len):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEncoding(3)\n",
    "        self.FourierList =  nn.ModuleList([FourierBlock(feature_dim,mlp_dim) for _ in range(num_f)])\n",
    "        self.channel_interaction = nn.Sequential(\n",
    "            nn.Linear(in_features=feature_dim,out_features=feature_dim*16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=feature_dim*16,out_features=1),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=seq_len,out_features=seq_len//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=seq_len//2,out_features=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x+self.pe(x)\n",
    "        residual = x\n",
    "        for Fblock in self.FourierList:\n",
    "            f_features = Fblock(x)\n",
    "        f_features = self.norm(f_features + residual)\n",
    "        y = self.channel_interaction(f_features).squeeze()\n",
    "        y = self.classifier(y)\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:26:00.355157800Z",
     "start_time": "2023-12-13T07:26:00.353158800Z"
    }
   },
   "id": "68cd19c1d8973303"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, loss,device=\"cuda\"):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        l = loss(y_hat, y.long()).sum()\n",
    "        test_l_sum += l.item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum/n, test_l_sum/n\n",
    "\n",
    "def train(net,train_iter,val_iter,loss,num_epochs,optimizer,device=\"cuda\"):\n",
    "    net.train()\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    val_acc_best = 1e-9\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            X = X.to(device)\n",
    "            #print(\"X:\",X.shape)\n",
    "            y = y.to(device)\n",
    "            #print(\"y:\",y.shape) \n",
    "            y_hat = net(X)\n",
    "            #print(\"y_hat:\",y_hat.shape)\n",
    "            #y_hat = y_hat.squeeze()\n",
    "            #print(\"y_hat_squeeze:\",y_hat.shape)\n",
    "            l=loss(y_hat,y.long()).sum()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        net.eval()\n",
    "        val_acc,val_l = evaluate_accuracy(val_iter,net,loss)\n",
    "        net.train()\n",
    "        train_loss.append(train_l_sum/n)\n",
    "        val_loss.append(val_l)\n",
    "        train_accuracy.append(train_acc_sum/n)\n",
    "        val_accuracy.append(val_acc)\n",
    "        if(val_acc>val_acc_best):\n",
    "            val_acc_best = val_acc\n",
    "            train_loss_best = train_l_sum/n\n",
    "            train_acc_best = train_acc_sum/n\n",
    "            epoch_best = epoch\n",
    "            torch.save(net.state_dict(),\"../weights/FNet_best.pt\")\n",
    "        print('epoch%d,loss%.4f,train acc %3f,val acc %.3f'%(epoch+1,train_l_sum/n,train_acc_sum/n,val_acc))\n",
    "    print(\"best weight was found in epoch%d,where the train loss is %.4f,train acc is %3f,val acc is %.3f\"%(epoch_best,train_loss_best,train_acc_best,val_acc_best))\n",
    "    return train_loss,val_loss,train_accuracy,val_accuracy\n",
    "\n",
    "\n",
    "def visualization(train_loss,val_loss,train_accuracy,val_accuracy):\n",
    "    plt.figure(1)\n",
    "    x1=np.linspace(0,len(train_loss),len(train_loss))\n",
    "    plt.plot(x1,train_loss,label='train_loss',linewidth=1.5)\n",
    "    plt.plot(x1,val_loss,label='test_loss',linewidth=1.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(2)\n",
    "    x2 = np.linspace(0,len(train_accuracy),len(train_accuracy))\n",
    "    plt.plot(x2,train_accuracy,label='train_accuracy',linewidth=1.5)\n",
    "    plt.plot(x2,val_accuracy,label='val_accuracy',linewidth=1.5)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:26:01.064615600Z",
     "start_time": "2023-12-13T07:26:01.064111900Z"
    }
   },
   "id": "46f1183abd8ea6ff"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "lr = 0.001\n",
    "epochs=150\n",
    "loss=nn.CrossEntropyLoss()\n",
    "net = FNet(3,3,32,100)\n",
    "net.to(device)\n",
    "optimizer=optim.Adam(net.parameters(),lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:26:02.369348700Z",
     "start_time": "2023-12-13T07:26:01.734612800Z"
    }
   },
   "id": "ce3d6a167f58de8f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.0020,train acc 0.768922,val acc 0.791\n",
      "epoch2,loss0.0019,train acc 0.795414,val acc 0.807\n",
      "epoch3,loss0.0019,train acc 0.803792,val acc 0.817\n",
      "epoch4,loss0.0019,train acc 0.811891,val acc 0.820\n",
      "epoch5,loss0.0019,train acc 0.816113,val acc 0.818\n",
      "epoch6,loss0.0019,train acc 0.821906,val acc 0.839\n",
      "epoch7,loss0.0019,train acc 0.825100,val acc 0.836\n",
      "epoch8,loss0.0018,train acc 0.827237,val acc 0.836\n",
      "epoch9,loss0.0018,train acc 0.830100,val acc 0.846\n",
      "epoch10,loss0.0018,train acc 0.833037,val acc 0.846\n",
      "epoch11,loss0.0018,train acc 0.833911,val acc 0.853\n",
      "epoch12,loss0.0018,train acc 0.837641,val acc 0.855\n",
      "epoch13,loss0.0018,train acc 0.838368,val acc 0.853\n",
      "epoch14,loss0.0018,train acc 0.841217,val acc 0.859\n",
      "epoch15,loss0.0018,train acc 0.842208,val acc 0.854\n",
      "epoch16,loss0.0018,train acc 0.842311,val acc 0.858\n",
      "epoch17,loss0.0018,train acc 0.843728,val acc 0.864\n",
      "epoch18,loss0.0018,train acc 0.845821,val acc 0.861\n",
      "epoch19,loss0.0018,train acc 0.847333,val acc 0.861\n",
      "epoch20,loss0.0018,train acc 0.849301,val acc 0.865\n",
      "epoch21,loss0.0018,train acc 0.847583,val acc 0.867\n",
      "epoch22,loss0.0018,train acc 0.848978,val acc 0.866\n",
      "epoch23,loss0.0018,train acc 0.851651,val acc 0.859\n",
      "epoch24,loss0.0018,train acc 0.852495,val acc 0.869\n",
      "epoch25,loss0.0018,train acc 0.852444,val acc 0.866\n",
      "epoch26,loss0.0018,train acc 0.852282,val acc 0.868\n",
      "epoch27,loss0.0018,train acc 0.853310,val acc 0.866\n",
      "epoch28,loss0.0018,train acc 0.854419,val acc 0.868\n",
      "epoch29,loss0.0018,train acc 0.855623,val acc 0.872\n",
      "epoch30,loss0.0018,train acc 0.855439,val acc 0.873\n",
      "epoch31,loss0.0018,train acc 0.855359,val acc 0.876\n",
      "epoch32,loss0.0018,train acc 0.856210,val acc 0.875\n",
      "epoch33,loss0.0017,train acc 0.856754,val acc 0.874\n",
      "epoch34,loss0.0017,train acc 0.857202,val acc 0.874\n",
      "epoch35,loss0.0017,train acc 0.858942,val acc 0.865\n",
      "epoch36,loss0.0017,train acc 0.857870,val acc 0.872\n",
      "epoch37,loss0.0017,train acc 0.858318,val acc 0.871\n",
      "epoch38,loss0.0017,train acc 0.859162,val acc 0.875\n",
      "epoch39,loss0.0017,train acc 0.858956,val acc 0.874\n",
      "epoch40,loss0.0017,train acc 0.858450,val acc 0.873\n",
      "epoch41,loss0.0017,train acc 0.859757,val acc 0.870\n",
      "epoch42,loss0.0017,train acc 0.860051,val acc 0.877\n",
      "epoch43,loss0.0017,train acc 0.859309,val acc 0.863\n",
      "epoch44,loss0.0017,train acc 0.858993,val acc 0.876\n",
      "epoch45,loss0.0017,train acc 0.859948,val acc 0.878\n",
      "epoch46,loss0.0017,train acc 0.861145,val acc 0.877\n",
      "epoch47,loss0.0017,train acc 0.861240,val acc 0.880\n",
      "epoch48,loss0.0017,train acc 0.861101,val acc 0.880\n",
      "epoch49,loss0.0017,train acc 0.860829,val acc 0.881\n",
      "epoch50,loss0.0017,train acc 0.861321,val acc 0.879\n",
      "epoch51,loss0.0017,train acc 0.861034,val acc 0.876\n",
      "epoch52,loss0.0017,train acc 0.861365,val acc 0.875\n",
      "epoch53,loss0.0017,train acc 0.861269,val acc 0.878\n",
      "epoch54,loss0.0017,train acc 0.861438,val acc 0.874\n",
      "epoch55,loss0.0017,train acc 0.861167,val acc 0.870\n",
      "epoch56,loss0.0017,train acc 0.861556,val acc 0.879\n",
      "epoch57,loss0.0017,train acc 0.862576,val acc 0.878\n",
      "epoch58,loss0.0017,train acc 0.862275,val acc 0.879\n",
      "epoch59,loss0.0017,train acc 0.862239,val acc 0.880\n",
      "epoch60,loss0.0017,train acc 0.863648,val acc 0.876\n",
      "epoch61,loss0.0017,train acc 0.863120,val acc 0.873\n",
      "epoch62,loss0.0017,train acc 0.863134,val acc 0.881\n",
      "epoch63,loss0.0017,train acc 0.863810,val acc 0.881\n",
      "epoch64,loss0.0017,train acc 0.864030,val acc 0.881\n",
      "epoch65,loss0.0017,train acc 0.863516,val acc 0.873\n",
      "epoch66,loss0.0017,train acc 0.864074,val acc 0.879\n",
      "epoch67,loss0.0017,train acc 0.864919,val acc 0.876\n",
      "epoch68,loss0.0017,train acc 0.865800,val acc 0.880\n",
      "epoch69,loss0.0017,train acc 0.864838,val acc 0.879\n",
      "epoch70,loss0.0017,train acc 0.865345,val acc 0.871\n",
      "epoch71,loss0.0017,train acc 0.865021,val acc 0.881\n",
      "epoch72,loss0.0017,train acc 0.865477,val acc 0.884\n",
      "epoch73,loss0.0017,train acc 0.865124,val acc 0.882\n",
      "epoch74,loss0.0017,train acc 0.865535,val acc 0.876\n",
      "epoch75,loss0.0017,train acc 0.865036,val acc 0.881\n",
      "epoch76,loss0.0017,train acc 0.864390,val acc 0.871\n",
      "epoch77,loss0.0017,train acc 0.865168,val acc 0.886\n",
      "epoch78,loss0.0017,train acc 0.865484,val acc 0.882\n",
      "epoch79,loss0.0017,train acc 0.866115,val acc 0.881\n",
      "epoch80,loss0.0017,train acc 0.865014,val acc 0.883\n",
      "epoch81,loss0.0017,train acc 0.866534,val acc 0.883\n",
      "epoch82,loss0.0017,train acc 0.868157,val acc 0.882\n",
      "epoch83,loss0.0017,train acc 0.865455,val acc 0.878\n",
      "epoch84,loss0.0017,train acc 0.865719,val acc 0.881\n",
      "epoch85,loss0.0017,train acc 0.867283,val acc 0.885\n",
      "epoch86,loss0.0017,train acc 0.867459,val acc 0.882\n",
      "epoch87,loss0.0017,train acc 0.865521,val acc 0.883\n",
      "epoch88,loss0.0017,train acc 0.866064,val acc 0.878\n",
      "epoch89,loss0.0017,train acc 0.867474,val acc 0.884\n",
      "epoch90,loss0.0017,train acc 0.866659,val acc 0.882\n",
      "epoch91,loss0.0017,train acc 0.866842,val acc 0.884\n",
      "epoch92,loss0.0017,train acc 0.867400,val acc 0.879\n",
      "epoch93,loss0.0017,train acc 0.867114,val acc 0.878\n",
      "epoch94,loss0.0017,train acc 0.867276,val acc 0.884\n",
      "epoch95,loss0.0017,train acc 0.866167,val acc 0.878\n",
      "epoch96,loss0.0017,train acc 0.867158,val acc 0.885\n",
      "epoch97,loss0.0017,train acc 0.867995,val acc 0.882\n",
      "epoch98,loss0.0017,train acc 0.867724,val acc 0.884\n",
      "epoch99,loss0.0017,train acc 0.867621,val acc 0.882\n",
      "epoch100,loss0.0017,train acc 0.867562,val acc 0.878\n",
      "epoch101,loss0.0017,train acc 0.867415,val acc 0.882\n",
      "epoch102,loss0.0017,train acc 0.869089,val acc 0.885\n",
      "epoch103,loss0.0017,train acc 0.867900,val acc 0.883\n",
      "epoch104,loss0.0017,train acc 0.867643,val acc 0.880\n",
      "epoch105,loss0.0017,train acc 0.867555,val acc 0.884\n",
      "epoch106,loss0.0017,train acc 0.868649,val acc 0.886\n",
      "epoch107,loss0.0017,train acc 0.868487,val acc 0.886\n",
      "epoch108,loss0.0017,train acc 0.868017,val acc 0.885\n",
      "epoch109,loss0.0017,train acc 0.867914,val acc 0.882\n",
      "epoch110,loss0.0017,train acc 0.868392,val acc 0.882\n",
      "epoch111,loss0.0017,train acc 0.867679,val acc 0.885\n",
      "epoch112,loss0.0017,train acc 0.868370,val acc 0.879\n",
      "epoch113,loss0.0017,train acc 0.867958,val acc 0.884\n",
      "epoch114,loss0.0017,train acc 0.868884,val acc 0.881\n",
      "epoch115,loss0.0017,train acc 0.868120,val acc 0.884\n",
      "epoch116,loss0.0017,train acc 0.869097,val acc 0.886\n",
      "epoch117,loss0.0017,train acc 0.869897,val acc 0.887\n",
      "epoch118,loss0.0017,train acc 0.869310,val acc 0.886\n",
      "epoch119,loss0.0017,train acc 0.868759,val acc 0.883\n",
      "epoch120,loss0.0017,train acc 0.869097,val acc 0.887\n",
      "epoch121,loss0.0017,train acc 0.868913,val acc 0.885\n",
      "epoch122,loss0.0017,train acc 0.870616,val acc 0.886\n",
      "epoch123,loss0.0017,train acc 0.869038,val acc 0.884\n",
      "epoch124,loss0.0017,train acc 0.870572,val acc 0.884\n",
      "epoch125,loss0.0017,train acc 0.869537,val acc 0.887\n",
      "epoch126,loss0.0017,train acc 0.869332,val acc 0.885\n",
      "epoch127,loss0.0017,train acc 0.869434,val acc 0.878\n",
      "epoch128,loss0.0017,train acc 0.870308,val acc 0.880\n",
      "epoch129,loss0.0017,train acc 0.870044,val acc 0.886\n",
      "epoch130,loss0.0017,train acc 0.868472,val acc 0.885\n",
      "epoch131,loss0.0017,train acc 0.870389,val acc 0.887\n",
      "epoch132,loss0.0017,train acc 0.870477,val acc 0.887\n",
      "epoch133,loss0.0017,train acc 0.870022,val acc 0.885\n",
      "epoch134,loss0.0017,train acc 0.871006,val acc 0.889\n",
      "epoch135,loss0.0017,train acc 0.870125,val acc 0.883\n",
      "epoch136,loss0.0017,train acc 0.869456,val acc 0.886\n",
      "epoch137,loss0.0017,train acc 0.870359,val acc 0.882\n",
      "epoch138,loss0.0017,train acc 0.870565,val acc 0.886\n",
      "epoch139,loss0.0017,train acc 0.870095,val acc 0.879\n",
      "epoch140,loss0.0017,train acc 0.869875,val acc 0.884\n",
      "epoch141,loss0.0017,train acc 0.869934,val acc 0.884\n",
      "epoch142,loss0.0017,train acc 0.870470,val acc 0.886\n",
      "epoch143,loss0.0017,train acc 0.870837,val acc 0.886\n",
      "epoch144,loss0.0017,train acc 0.869941,val acc 0.889\n",
      "epoch145,loss0.0017,train acc 0.871968,val acc 0.885\n",
      "epoch146,loss0.0017,train acc 0.871094,val acc 0.887\n",
      "epoch147,loss0.0017,train acc 0.871520,val acc 0.887\n",
      "epoch148,loss0.0017,train acc 0.870683,val acc 0.883\n",
      "epoch149,loss0.0017,train acc 0.870646,val acc 0.886\n",
      "epoch150,loss0.0017,train acc 0.871277,val acc 0.888\n",
      "best weight was found in epoch143,where the train loss is 0.0017,train acc is 0.869941,val acc is 0.889\n"
     ]
    }
   ],
   "source": [
    "train_loss,val_loss,train_accuracy,val_accuracy=train(net,train_loader,val_loader,loss,epochs,optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:33:38.003443500Z",
     "start_time": "2023-12-13T07:26:03.246921300Z"
    }
   },
   "id": "525ea352a1201a1a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上的准确率为:0.869\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"../weights/FNet_best.pt\"))\n",
    "test_acc,_ = evaluate_accuracy(test_loader,net,loss)\n",
    "print(\"在测试集上的准确率为:%.3f\"%(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:34:35.334691100Z",
     "start_time": "2023-12-13T07:34:34.324545700Z"
    }
   },
   "id": "dd344e433a388582"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision: 0.8386743065613255  test recall: 0.8440805326600099  test f1_score: 0.8407806587542624\n"
     ]
    }
   ],
   "source": [
    "def evaluate_PR(data_iter, net, loss,device=\"cuda\"):\n",
    "    n = 0\n",
    "    precision_sum,recall_sum,f1_score_sum = 0.0,0.0,0.0\n",
    "    test_l_sum = 0.0\n",
    "    for X, y in data_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y.long()).sum()\n",
    "        y_hat = y_hat.argmax(dim=1)\n",
    "        y_hat = y_hat.squeeze().cpu().detach().numpy()\n",
    "        y = y.cpu().detach().numpy()\n",
    "        precision_sum += metrics.precision_score(y,y_hat)\n",
    "        recall_sum += metrics.recall_score(y,y_hat)\n",
    "        f1_score_sum += metrics.f1_score(y,y_hat)\n",
    "        test_l_sum += l.item()\n",
    "        #n += y.shape[0]\n",
    "        n+=1\n",
    "    return precision_sum/n,recall_sum/n,f1_score_sum/n\n",
    "test_precision,test_recall,test_f1_score = evaluate_PR(test_loader,net,loss)\n",
    "print(\"test precision:\",test_precision,\" test recall:\",test_recall,\" test f1_score:\",test_f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T07:34:38.798022500Z",
     "start_time": "2023-12-13T07:34:37.511809900Z"
    }
   },
   "id": "6c491cfdb90045a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6e99d341cdeafd80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
